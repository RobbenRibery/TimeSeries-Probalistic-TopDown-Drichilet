{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "sys.path.insert(0, '../src/')\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from hts.hierarchy import HierarchyTree \n",
    "from datetime import datetime \n",
    "import torch \n",
    "\n",
    "\n",
    "import proption_model \n",
    "import hierachy_encoding \n",
    "import utils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_validation = pd.read_csv('../data/sales_train_validation.csv')\n",
    "sales_train_evaluation = pd.read_csv('../data/sales_train_evaluation.csv') \n",
    "calender = pd.read_csv('../data/calendar.csv') \n",
    "date_to_d = dict(zip(calender.date, calender.d)) \n",
    "d_to_date = dict(zip(calender.d, calender.date)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent nodes sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1913,)\n"
     ]
    }
   ],
   "source": [
    "parent_sales = sales_train_validation[sales_train_validation.columns[6:]].sum(axis=0).values\n",
    "print(parent_sales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hie EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_train_evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30490, 1947)\n",
      "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(sales_train_evaluation.shape)\n",
    "print(sales_train_evaluation.columns[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_list = ['state_id','store_id','cat_id','dept_id','item_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fully encoded hierachy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">CA</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">CA_1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">FOODS</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">FOODS_1</th>\n",
       "      <th>FOODS_1_001</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_002</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_003</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_004</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_005</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">WI</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">WI_3</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">HOUSEHOLD</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">HOUSEHOLD_2</th>\n",
       "      <th>HOUSEHOLD_2_512</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_513</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_514</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_515</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_516</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         id\n",
       "state_id store_id cat_id    dept_id     item_id            \n",
       "CA       CA_1     FOODS     FOODS_1     FOODS_1_001       1\n",
       "                                        FOODS_1_002       1\n",
       "                                        FOODS_1_003       1\n",
       "                                        FOODS_1_004       1\n",
       "                                        FOODS_1_005       1\n",
       "...                                                      ..\n",
       "WI       WI_3     HOUSEHOLD HOUSEHOLD_2 HOUSEHOLD_2_512   1\n",
       "                                        HOUSEHOLD_2_513   1\n",
       "                                        HOUSEHOLD_2_514   1\n",
       "                                        HOUSEHOLD_2_515   1\n",
       "                                        HOUSEHOLD_2_516   1\n",
       "\n",
       "[30490 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierachy_lookup = sales_train_evaluation.groupby(groupby_list[:]).count()[['id']]\n",
    "hierachy_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierachy_lookup.loc[('CA','CA_1','FOODS','FOODS_1')].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partially encoded hierachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">FOODS</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">FOODS_1</th>\n",
       "      <th>FOODS_1_001</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_002</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_003</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_004</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_005</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id\n",
       "cat_id dept_id item_id        \n",
       "FOODS  FOODS_1 FOODS_1_001  10\n",
       "               FOODS_1_002  10\n",
       "               FOODS_1_003  10\n",
       "               FOODS_1_004  10\n",
       "               FOODS_1_005  10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierachy_lookup_2 = sales_train_evaluation.groupby(groupby_list[2:]).count()[['id']]\n",
    "hierachy_lookup_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">FOODS</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">FOODS_1</th>\n",
       "      <th>FOODS_1_001</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_002</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_003</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_004</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_005</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">HOUSEHOLD</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">HOUSEHOLD_2</th>\n",
       "      <th>HOUSEHOLD_2_512</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_513</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_514</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_515</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD_2_516</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3049 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id\n",
       "cat_id    dept_id     item_id            \n",
       "FOODS     FOODS_1     FOODS_1_001      10\n",
       "                      FOODS_1_002      10\n",
       "                      FOODS_1_003      10\n",
       "                      FOODS_1_004      10\n",
       "                      FOODS_1_005      10\n",
       "...                                    ..\n",
       "HOUSEHOLD HOUSEHOLD_2 HOUSEHOLD_2_512  10\n",
       "                      HOUSEHOLD_2_513  10\n",
       "                      HOUSEHOLD_2_514  10\n",
       "                      HOUSEHOLD_2_515  10\n",
       "                      HOUSEHOLD_2_516  10\n",
       "\n",
       "[3049 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierachy_lookup_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TS encoding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierachy_lookup_2 = hierachy_lookup_2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOODS_3        823\n",
       "HOUSEHOLD_1    532\n",
       "HOUSEHOLD_2    515\n",
       "HOBBIES_1      416\n",
       "FOODS_2        398\n",
       "FOODS_1        216\n",
       "HOBBIES_2      149\n",
       "Name: dept_id, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierachy_lookup_2.dept_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FOODS': 0, 'HOBBIES': 1, 'HOUSEHOLD': 2}\n",
      "{'FOODS_1': 3, 'FOODS_2': 4, 'FOODS_3': 5, 'HOBBIES_1': 6, 'HOBBIES_2': 7, 'HOUSEHOLD_1': 8, 'HOUSEHOLD_2': 9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hierachy_with_encoded_columns = hierachy_encoding.hie_encoder(hierachy_lookup_2, ['cat_id','dept_id','item_id'])\n",
    "\n",
    "root_to_index = {'root':0}\n",
    "\n",
    "cat_to_dix = dict(zip(hierachy_with_encoded_columns.cat_id,hierachy_with_encoded_columns.cat_id_))\n",
    "print(cat_to_dix)\n",
    "\n",
    "\n",
    "dep_to_dix = dict(zip(hierachy_with_encoded_columns.dept_id,hierachy_with_encoded_columns.dept_id_))\n",
    "print(dep_to_dix)\n",
    "\n",
    "item_to_dix = dict(zip(hierachy_with_encoded_columns.item_id,hierachy_with_encoded_columns.item_id_))\n",
    "#print(item_to_dix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n"
     ]
    }
   ],
   "source": [
    "ts_to_index = {}\n",
    "for dic in [root_to_index, cat_to_dix, dep_to_dix, item_to_dix]:\n",
    "    ts_to_index.update(dic)\n",
    "print(len(ts_to_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3059\n"
     ]
    }
   ],
   "source": [
    "inde_to_ts = dict(zip(ts_to_index.values(), ts_to_index.keys()))\n",
    "print(len(inde_to_ts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'FOODS_1': 0,\n",
       "             'FOODS_2': 0,\n",
       "             'FOODS_3': 0,\n",
       "             'HOBBIES_1': 1,\n",
       "             'HOBBIES_2': 1,\n",
       "             'HOUSEHOLD_1': 2,\n",
       "             'HOUSEHOLD_2': 2})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_parent_index = hierachy_encoding.get_parent_index(dep_to_dix, cat_to_dix)\n",
    "dep_parent_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_parent_index = hierachy_encoding.get_parent_index(item_to_dix, dep_to_dix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- Need to constrcut a grpah here, proceeed to next step first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'root': [0, 1, 2]})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierachy_encoding.get_children_index(cat_to_dix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick a particular familry to construct one training data points \n",
    "    - Parent node - 0 \n",
    "    - Children node - 1, 2, 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1913)\n"
     ]
    }
   ],
   "source": [
    "# collect parent sales \n",
    "yp = sales_train_validation.sum(axis=0)[6:].values.reshape(-1,1)# np array \n",
    "yp = yp.T \n",
    "print(yp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>d_5</th>\n",
       "      <th>d_6</th>\n",
       "      <th>d_7</th>\n",
       "      <th>d_8</th>\n",
       "      <th>d_9</th>\n",
       "      <th>d_10</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FOODS</th>\n",
       "      <td>23178</td>\n",
       "      <td>22758</td>\n",
       "      <td>17174</td>\n",
       "      <td>18878</td>\n",
       "      <td>14603</td>\n",
       "      <td>22093</td>\n",
       "      <td>20490</td>\n",
       "      <td>27751</td>\n",
       "      <td>24862</td>\n",
       "      <td>18901</td>\n",
       "      <td>...</td>\n",
       "      <td>28682</td>\n",
       "      <td>32007</td>\n",
       "      <td>34497</td>\n",
       "      <td>26151</td>\n",
       "      <td>24948</td>\n",
       "      <td>23632</td>\n",
       "      <td>23317</td>\n",
       "      <td>26704</td>\n",
       "      <td>31927</td>\n",
       "      <td>32654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOBBIES</th>\n",
       "      <td>3764</td>\n",
       "      <td>3357</td>\n",
       "      <td>2682</td>\n",
       "      <td>2669</td>\n",
       "      <td>1814</td>\n",
       "      <td>3220</td>\n",
       "      <td>2944</td>\n",
       "      <td>3986</td>\n",
       "      <td>2899</td>\n",
       "      <td>2615</td>\n",
       "      <td>...</td>\n",
       "      <td>3786</td>\n",
       "      <td>4634</td>\n",
       "      <td>4820</td>\n",
       "      <td>3323</td>\n",
       "      <td>3787</td>\n",
       "      <td>3472</td>\n",
       "      <td>3353</td>\n",
       "      <td>4085</td>\n",
       "      <td>4787</td>\n",
       "      <td>4683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD</th>\n",
       "      <td>5689</td>\n",
       "      <td>5634</td>\n",
       "      <td>3927</td>\n",
       "      <td>3865</td>\n",
       "      <td>2729</td>\n",
       "      <td>3898</td>\n",
       "      <td>4576</td>\n",
       "      <td>6195</td>\n",
       "      <td>4975</td>\n",
       "      <td>4056</td>\n",
       "      <td>...</td>\n",
       "      <td>9321</td>\n",
       "      <td>11721</td>\n",
       "      <td>12323</td>\n",
       "      <td>8585</td>\n",
       "      <td>8835</td>\n",
       "      <td>8239</td>\n",
       "      <td>8363</td>\n",
       "      <td>9728</td>\n",
       "      <td>12248</td>\n",
       "      <td>12458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             d_1    d_2    d_3    d_4    d_5    d_6    d_7    d_8    d_9  \\\n",
       "cat_id                                                                     \n",
       "FOODS      23178  22758  17174  18878  14603  22093  20490  27751  24862   \n",
       "HOBBIES     3764   3357   2682   2669   1814   3220   2944   3986   2899   \n",
       "HOUSEHOLD   5689   5634   3927   3865   2729   3898   4576   6195   4975   \n",
       "\n",
       "            d_10  ...  d_1904  d_1905  d_1906  d_1907  d_1908  d_1909  d_1910  \\\n",
       "cat_id            ...                                                           \n",
       "FOODS      18901  ...   28682   32007   34497   26151   24948   23632   23317   \n",
       "HOBBIES     2615  ...    3786    4634    4820    3323    3787    3472    3353   \n",
       "HOUSEHOLD   4056  ...    9321   11721   12323    8585    8835    8239    8363   \n",
       "\n",
       "           d_1911  d_1912  d_1913  \n",
       "cat_id                             \n",
       "FOODS       26704   31927   32654  \n",
       "HOBBIES      4085    4787    4683  \n",
       "HOUSEHOLD    9728   12248   12458  \n",
       "\n",
       "[3 rows x 1913 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_train_validation.groupby('cat_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1913)\n"
     ]
    }
   ],
   "source": [
    "# collect the proportinos \n",
    "ac = sales_train_validation.groupby('cat_id').sum().values/yp\n",
    "print(ac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1913)\n"
     ]
    }
   ],
   "source": [
    "# collect the embedding layer input \n",
    "ec = np.array(hierachy_encoding.get_children_index(cat_to_dix)['root']).reshape(-1,1)\n",
    "ec = np.repeat(ec, 1913, axis=1)\n",
    "print(ec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1913, 3)\n",
      "0\n",
      "(1913, 3)\n",
      "1\n",
      "(1913, 3)\n",
      "2\n",
      "(1913, 3)\n"
     ]
    }
   ],
   "source": [
    "if ac.shape[0] == ec.shape[0]: \n",
    "\n",
    "    input = np.empty(\n",
    "        \n",
    "        (ac.shape[0], ac.shape[1], 3), \n",
    "    \n",
    "    )\n",
    "    print(input.shape)\n",
    "\n",
    "    for c in range(input.shape[0]): \n",
    "        print(c)\n",
    "        # print(yp.T.shape)\n",
    "        # print(ac[c].reshape(-1,1).shape)\n",
    "        # print(ec[c].reshape(-1,1).shape)\n",
    "        input[c] = np.concatenate(\n",
    "            [\n",
    "                ac[c].reshape(-1,1), \n",
    "                yp.T, \n",
    "                ## ---- PLACE HOLDER FOR COVARIATES X ---- ## \n",
    "                ## ---- PLACE HOLDER FOR COVARIATES X ---- ## \n",
    "                ec[c].reshape(-1,1),\n",
    "            ], \n",
    "            axis = 1 \n",
    "        )\n",
    "        print(input[c].shape)    \n",
    "else: \n",
    "    raise \"size of children in embedding does not agree with size of the children in proportions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1913, 3)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Time batched input --------\n",
      "(1893, 3, 21, 3)\n",
      "-------- X, y split ------------\n",
      "X input shape is (1893, 3, 14, 3)\n",
      "y input shape is (1893, 3, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "## time batching \n",
    "# dimension about the dataset\n",
    "print('------- Time batched input --------')\n",
    "History = 14\n",
    "Forward = 7\n",
    "\n",
    "number_observations = input.shape[1] - (History + Forward) + 1\n",
    "\n",
    "input_time_batched = np.empty(\n",
    "    (number_observations,input.shape[0], History + Forward, input.shape[-1])\n",
    ")\n",
    "\n",
    "for i in range(number_observations):\n",
    "\n",
    "    input_time_batched[i] = np.array(input[:, i:i + History + Forward, :])\n",
    "\n",
    "print(input_time_batched.shape)\n",
    "# print(input_time_batched[0][0])\n",
    "\n",
    "print(\"-------- X, y split ------------\")\n",
    "input_array = np.empty((\n",
    "    number_observations,\n",
    "    input.shape[0],\n",
    "    History,\n",
    "    input.shape[-1])\n",
    ")\n",
    "\n",
    "target_array = np.empty((\n",
    "    number_observations,\n",
    "    input.shape[0],\n",
    "    Forward,\n",
    "    1)\n",
    ")\n",
    "\n",
    "# print(input_tensor.shape)\n",
    "# print(target_tensor.shape)\n",
    "\n",
    "for i in range(input_time_batched.shape[0]):\n",
    "\n",
    "    input_array[i] = input_time_batched[i, :, :History, :]\n",
    "\n",
    "    #print(input_array[i,0,-1,0])\n",
    "\n",
    "    target_2d = input_time_batched[i, :, History:, 0]\n",
    "    \n",
    "    target_array[i] = target_2d.reshape(\n",
    "        target_2d.shape[0], target_2d.shape[1], 1\n",
    "    )\n",
    "\n",
    "    #print(target_array[i,0,0,0])\n",
    "    #print()\n",
    "\n",
    "print(f\"X input shape is {input_array.shape}\")\n",
    "print(f\"y input shape is {target_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]../src/proption_model.py:242: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "The batch loss for Iteration: 0 - Batch: 0 is 9.312482833862305\n",
      "The batch loss for Iteration: 0 - Batch: 1 is 9.435872554779053\n",
      "The batch loss for Iteration: 0 - Batch: 2 is 6.390219211578369\n",
      "The batch loss for Iteration: 0 - Batch: 3 is 4.08116352558136\n",
      "The batch loss for Iteration: 0 - Batch: 4 is 2.835453820228577\n",
      "The batch loss for Iteration: 0 - Batch: 5 is 2.191328179836273\n",
      "The batch loss for Iteration: 0 - Batch: 6 is 1.8073744279997688\n",
      "The batch loss for Iteration: 0 - Batch: 7 is 1.5470279989498001\n",
      "The batch loss for Iteration: 0 - Batch: 8 is 1.3459512319593203\n",
      "The batch loss for Iteration: 0 - Batch: 9 is 1.1678713950160005\n",
      "The batch loss for Iteration: 0 - Batch: 10 is 1.0278927990220068\n",
      "The batch loss for Iteration: 0 - Batch: 11 is 0.9085311666828235\n",
      "The batch loss for Iteration: 0 - Batch: 12 is 0.7994557877053194\n",
      "The batch loss for Iteration: 0 - Batch: 13 is 0.7103507281745429\n",
      "The batch loss for Iteration: 0 - Batch: 14 is 0.6323125664486153\n",
      "The batch loss for Iteration: 0 - Batch: 15 is 0.5579385627315296\n",
      "The batch loss for Iteration: 0 - Batch: 16 is 0.530599761646202\n",
      "The batch loss for Iteration: 0 - Batch: 17 is 0.5040032691779629\n",
      "The batch loss for Iteration: 0 - Batch: 18 is 0.48368352247446716\n",
      "The batch loss for Iteration: 0 - Batch: 19 is 0.47113561312018326\n",
      "The batch loss for Iteration: 0 - Batch: 20 is 0.45892013807129217\n",
      "The batch loss for Iteration: 0 - Batch: 21 is 0.4496291179092686\n",
      "The batch loss for Iteration: 0 - Batch: 22 is 0.4438223756682698\n",
      "The batch loss for Iteration: 0 - Batch: 23 is 0.4254249529595666\n",
      "The batch loss for Iteration: 0 - Batch: 24 is 0.40657739240549207\n",
      "The batch loss for Iteration: 0 - Batch: 25 is 0.3891007091749258\n",
      "The batch loss for Iteration: 0 - Batch: 26 is 0.37618296818287483\n",
      "The batch loss for Iteration: 0 - Batch: 27 is 0.36339522879618663\n",
      "The batch loss for Iteration: 0 - Batch: 28 is 0.353946965823673\n",
      "The batch loss for Iteration: 0 - Batch: 29 is 0.3455097291830547\n",
      "The batch loss for Iteration: 0 - Batch: 30 is 0.3365635782607248\n",
      "The batch loss for Iteration: 0 - Batch: 31 is 0.32893530996556036\n",
      "The batch loss for Iteration: 0 - Batch: 32 is 0.32205754111268153\n",
      "The batch loss for Iteration: 0 - Batch: 33 is 0.31279406290137823\n",
      "The batch loss for Iteration: 0 - Batch: 34 is 0.30492788988660746\n",
      "The batch loss for Iteration: 0 - Batch: 35 is 0.2993501045191484\n",
      "The batch loss for Iteration: 0 - Batch: 36 is 0.2906414324579051\n",
      "The batch loss for Iteration: 0 - Batch: 37 is 0.28417706817529026\n",
      "The batch loss for Iteration: 0 - Batch: 38 is 0.27842742365907946\n",
      "The batch loss for Iteration: 0 - Batch: 39 is 0.2725288704440771\n",
      "The batch loss for Iteration: 0 - Batch: 40 is 0.26657740041348454\n",
      "The batch loss for Iteration: 0 - Batch: 41 is 0.26172832453173617\n",
      "The batch loss for Iteration: 0 - Batch: 42 is 0.25353686841974016\n",
      "The batch loss for Iteration: 0 - Batch: 43 is 0.24749700969652907\n",
      "The batch loss for Iteration: 0 - Batch: 44 is 0.23908054403600792\n",
      "The batch loss for Iteration: 0 - Batch: 45 is 0.23179780131926955\n",
      "The batch loss for Iteration: 0 - Batch: 46 is 0.222227393034999\n",
      "The batch loss for Iteration: 0 - Batch: 47 is 0.2126858494853788\n",
      "The batch loss for Iteration: 0 - Batch: 48 is 0.20424105736304266\n",
      "The batch loss for Iteration: 0 - Batch: 49 is 0.1993206075852491\n",
      "The batch loss for Iteration: 0 - Batch: 50 is 0.19341548855013588\n",
      "The batch loss for Iteration: 0 - Batch: 51 is 0.18977830665518589\n",
      "The batch loss for Iteration: 0 - Batch: 52 is 0.18602196635262952\n",
      "The batch loss for Iteration: 0 - Batch: 53 is 0.18347945335865332\n",
      "The batch loss for Iteration: 0 - Batch: 54 is 0.1823555365698555\n",
      "The batch loss for Iteration: 0 - Batch: 55 is 0.1805797242967448\n",
      "The batch loss for Iteration: 0 - Batch: 56 is 0.17606202451779857\n",
      "The batch loss for Iteration: 0 - Batch: 57 is 0.17429952643527288\n",
      "The batch loss for Iteration: 0 - Batch: 58 is 0.17177958892937667\n",
      "The batch loss for Iteration: 0 - Batch: 59 is 0.17081540352886854\n",
      "The batch loss for Iteration: 0 - Batch: 60 is 0.16982400507333922\n",
      "The batch loss for Iteration: 0 - Batch: 61 is 0.16805149472057868\n",
      "The batch loss for Iteration: 0 - Batch: 62 is 0.16502548042370216\n",
      "The batch loss for Iteration: 0 - Batch: 63 is 0.16252991196752734\n",
      "The batch loss for Iteration: 0 - Batch: 64 is 0.15810680338010438\n",
      "The batch loss for Iteration: 0 - Batch: 65 is 0.15511801747786053\n",
      "The batch loss for Iteration: 0 - Batch: 66 is 0.15450711079917545\n",
      "The batch loss for Iteration: 0 - Batch: 67 is 0.15457454075832025\n",
      "The batch loss for Iteration: 0 - Batch: 68 is 0.15449439827808706\n",
      "The batch loss for Iteration: 0 - Batch: 69 is 0.15524222671193863\n",
      "The batch loss for Iteration: 0 - Batch: 70 is 0.15593992519535413\n",
      "The batch loss for Iteration: 0 - Batch: 71 is 0.15799987134670737\n",
      "The batch loss for Iteration: 0 - Batch: 72 is 0.15737050034375707\n",
      "The batch loss for Iteration: 0 - Batch: 73 is 0.15280447688964194\n",
      "The batch loss for Iteration: 0 - Batch: 74 is 0.14908573286894522\n",
      "The batch loss for Iteration: 0 - Batch: 75 is 0.14536980229213095\n",
      "The batch loss for Iteration: 0 - Batch: 76 is 0.14105193999907595\n",
      "The batch loss for Iteration: 0 - Batch: 77 is 0.13671241270012088\n",
      "The batch loss for Iteration: 0 - Batch: 78 is 0.13229800302601394\n",
      "The batch loss for Iteration: 0 - Batch: 79 is 0.13031509531522995\n",
      "The batch loss for Iteration: 0 - Batch: 80 is 0.12723340259817223\n",
      "The batch loss for Iteration: 0 - Batch: 81 is 0.12460245612031402\n",
      "The batch loss for Iteration: 0 - Batch: 82 is 0.12303753525410864\n",
      "The batch loss for Iteration: 0 - Batch: 83 is 0.12174251200547231\n",
      "The batch loss for Iteration: 0 - Batch: 84 is 0.12149476753985068\n",
      "The batch loss for Iteration: 0 - Batch: 85 is 0.12107625670064734\n",
      "The batch loss for Iteration: 0 - Batch: 86 is 0.11924522944643456\n",
      "The batch loss for Iteration: 0 - Batch: 87 is 0.1201066069788502\n",
      "The batch loss for Iteration: 0 - Batch: 88 is 0.12004311063873051\n",
      "The batch loss for Iteration: 0 - Batch: 89 is 0.11927668597731185\n",
      "The batch loss for Iteration: 0 - Batch: 90 is 0.11816611311226463\n",
      "The batch loss for Iteration: 0 - Batch: 91 is 0.1175912582379093\n",
      "The batch loss for Iteration: 0 - Batch: 92 is 0.11690312427805906\n",
      "The batch loss for Iteration: 0 - Batch: 93 is 0.11632826795418709\n",
      "The batch loss for Iteration: 0 - Batch: 94 is 0.11458767368816908\n",
      "The batch loss for Iteration: 0 - Batch: 95 is 0.11309481545811141\n",
      "The batch loss for Iteration: 0 - Batch: 96 is 0.11126934630810655\n",
      "The batch loss for Iteration: 0 - Batch: 97 is 0.11079644539963827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:07<06:27,  7.91s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 0 - Batch: 98 is 0.10994724787636352\n",
      "The batch loss for Iteration: 0 - Batch: 99 is 0.10807270093335347\n",
      "Iteration: 1\n",
      "The batch loss for Iteration: 1 - Batch: 0 is 9.306734085083008\n",
      "The batch loss for Iteration: 1 - Batch: 1 is 9.432132244110107\n",
      "The batch loss for Iteration: 1 - Batch: 2 is 6.388735294342041\n",
      "The batch loss for Iteration: 1 - Batch: 3 is 4.080789685249329\n",
      "The batch loss for Iteration: 1 - Batch: 4 is 2.835384392738342\n",
      "The batch loss for Iteration: 1 - Batch: 5 is 2.191313111782074\n",
      "The batch loss for Iteration: 1 - Batch: 6 is 1.807337670666831\n",
      "The batch loss for Iteration: 1 - Batch: 7 is 1.546934235734599\n",
      "The batch loss for Iteration: 1 - Batch: 8 is 1.345937317018471\n",
      "The batch loss for Iteration: 1 - Batch: 9 is 1.1678651397829019\n",
      "The batch loss for Iteration: 1 - Batch: 10 is 1.0278874619928704\n",
      "The batch loss for Iteration: 1 - Batch: 11 is 0.9085283377446044\n",
      "The batch loss for Iteration: 1 - Batch: 12 is 0.7994717091985032\n",
      "The batch loss for Iteration: 1 - Batch: 13 is 0.710384835307566\n",
      "The batch loss for Iteration: 1 - Batch: 14 is 0.6323411616686164\n",
      "The batch loss for Iteration: 1 - Batch: 15 is 0.5579493502341408\n",
      "The batch loss for Iteration: 1 - Batch: 16 is 0.5305861471889224\n",
      "The batch loss for Iteration: 1 - Batch: 17 is 0.5039543522662465\n",
      "The batch loss for Iteration: 1 - Batch: 18 is 0.48361409031124775\n",
      "The batch loss for Iteration: 1 - Batch: 19 is 0.47106190339861903\n",
      "The batch loss for Iteration: 1 - Batch: 20 is 0.45885473008011\n",
      "The batch loss for Iteration: 1 - Batch: 21 is 0.4495844432418339\n",
      "The batch loss for Iteration: 1 - Batch: 22 is 0.4438099843380451\n",
      "The batch loss for Iteration: 1 - Batch: 23 is 0.42541871460824215\n",
      "The batch loss for Iteration: 1 - Batch: 24 is 0.40657340446811874\n",
      "The batch loss for Iteration: 1 - Batch: 25 is 0.3890988685227747\n",
      "The batch loss for Iteration: 1 - Batch: 26 is 0.37618109862575305\n",
      "The batch loss for Iteration: 1 - Batch: 27 is 0.3633918582259791\n",
      "The batch loss for Iteration: 1 - Batch: 28 is 0.35394629054665283\n",
      "The batch loss for Iteration: 1 - Batch: 29 is 0.34551317169050366\n",
      "The batch loss for Iteration: 1 - Batch: 30 is 0.3365693498278754\n",
      "The batch loss for Iteration: 1 - Batch: 31 is 0.32893712945476516\n",
      "The batch loss for Iteration: 1 - Batch: 32 is 0.32205490862109903\n",
      "The batch loss for Iteration: 1 - Batch: 33 is 0.31279106835371684\n",
      "The batch loss for Iteration: 1 - Batch: 34 is 0.3049225454954438\n",
      "The batch loss for Iteration: 1 - Batch: 35 is 0.2993456910203677\n",
      "The batch loss for Iteration: 1 - Batch: 36 is 0.29064144204906184\n",
      "The batch loss for Iteration: 1 - Batch: 37 is 0.28417794681192793\n",
      "The batch loss for Iteration: 1 - Batch: 38 is 0.2784273483754736\n",
      "The batch loss for Iteration: 1 - Batch: 39 is 0.2725273665249386\n",
      "The batch loss for Iteration: 1 - Batch: 40 is 0.2665755494252939\n",
      "The batch loss for Iteration: 1 - Batch: 41 is 0.2617250334266067\n",
      "The batch loss for Iteration: 1 - Batch: 42 is 0.25353557206642546\n",
      "The batch loss for Iteration: 1 - Batch: 43 is 0.2474961999549676\n",
      "The batch loss for Iteration: 1 - Batch: 44 is 0.23907916970494547\n",
      "The batch loss for Iteration: 1 - Batch: 45 is 0.23179654825153626\n",
      "The batch loss for Iteration: 1 - Batch: 46 is 0.22222592571703734\n",
      "The batch loss for Iteration: 1 - Batch: 47 is 0.21268490497836803\n",
      "The batch loss for Iteration: 1 - Batch: 48 is 0.20424158304414158\n",
      "The batch loss for Iteration: 1 - Batch: 49 is 0.19932145733226958\n",
      "The batch loss for Iteration: 1 - Batch: 50 is 0.19341599139874854\n",
      "The batch loss for Iteration: 1 - Batch: 51 is 0.18977749103027003\n",
      "The batch loss for Iteration: 1 - Batch: 52 is 0.18601928787293626\n",
      "The batch loss for Iteration: 1 - Batch: 53 is 0.1834773198021898\n",
      "The batch loss for Iteration: 1 - Batch: 54 is 0.18235513364772626\n",
      "The batch loss for Iteration: 1 - Batch: 55 is 0.18058039829764705\n",
      "The batch loss for Iteration: 1 - Batch: 56 is 0.17606165152642356\n",
      "The batch loss for Iteration: 1 - Batch: 57 is 0.17429845123144627\n",
      "The batch loss for Iteration: 1 - Batch: 58 is 0.17177830991580875\n",
      "The batch loss for Iteration: 1 - Batch: 59 is 0.17081344307419902\n",
      "The batch loss for Iteration: 1 - Batch: 60 is 0.16982305052843205\n",
      "The batch loss for Iteration: 1 - Batch: 61 is 0.1680508025235653\n",
      "The batch loss for Iteration: 1 - Batch: 62 is 0.16502459145056936\n",
      "The batch loss for Iteration: 1 - Batch: 63 is 0.16253000238545048\n",
      "The batch loss for Iteration: 1 - Batch: 64 is 0.15810670206776142\n",
      "The batch loss for Iteration: 1 - Batch: 65 is 0.1551185794776484\n",
      "The batch loss for Iteration: 1 - Batch: 66 is 0.1545069199120017\n",
      "The batch loss for Iteration: 1 - Batch: 67 is 0.15457422940946533\n",
      "The batch loss for Iteration: 1 - Batch: 68 is 0.15449388237520936\n",
      "The batch loss for Iteration: 1 - Batch: 69 is 0.15524185149608977\n",
      "The batch loss for Iteration: 1 - Batch: 70 is 0.15593991991062384\n",
      "The batch loss for Iteration: 1 - Batch: 71 is 0.1579999772371213\n",
      "The batch loss for Iteration: 1 - Batch: 72 is 0.15737056711446934\n",
      "The batch loss for Iteration: 1 - Batch: 73 is 0.15280451645442114\n",
      "The batch loss for Iteration: 1 - Batch: 74 is 0.14908527563280372\n",
      "The batch loss for Iteration: 1 - Batch: 75 is 0.14536884260154953\n",
      "The batch loss for Iteration: 1 - Batch: 76 is 0.14105087477819966\n",
      "The batch loss for Iteration: 1 - Batch: 77 is 0.13671134755637615\n",
      "The batch loss for Iteration: 1 - Batch: 78 is 0.13229776017847272\n",
      "The batch loss for Iteration: 1 - Batch: 79 is 0.1303149015447724\n",
      "The batch loss for Iteration: 1 - Batch: 80 is 0.1272331294095335\n",
      "The batch loss for Iteration: 1 - Batch: 81 is 0.1246021736645551\n",
      "The batch loss for Iteration: 1 - Batch: 82 is 0.12303733652014313\n",
      "The batch loss for Iteration: 1 - Batch: 83 is 0.12174240746020074\n",
      "The batch loss for Iteration: 1 - Batch: 84 is 0.12149462045383438\n",
      "The batch loss for Iteration: 1 - Batch: 85 is 0.12107625499034481\n",
      "The batch loss for Iteration: 1 - Batch: 86 is 0.11924541577692971\n",
      "The batch loss for Iteration: 1 - Batch: 87 is 0.1201067174683236\n",
      "The batch loss for Iteration: 1 - Batch: 88 is 0.12004309044930173\n",
      "The batch loss for Iteration: 1 - Batch: 89 is 0.11927670694574745\n",
      "The batch loss for Iteration: 1 - Batch: 90 is 0.11816612382262454\n",
      "The batch loss for Iteration: 1 - Batch: 91 is 0.1175912790863766\n",
      "The batch loss for Iteration: 1 - Batch: 92 is 0.11690313475679866\n",
      "The batch loss for Iteration: 1 - Batch: 93 is 0.11632831879302029\n",
      "The batch loss for Iteration: 1 - Batch: 94 is 0.11458770433934573\n",
      "The batch loss for Iteration: 1 - Batch: 95 is 0.11309480584328706\n",
      "The batch loss for Iteration: 1 - Batch: 96 is 0.11126928721882073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:15<06:18,  7.88s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 1 - Batch: 97 is 0.1107964058712041\n",
      "The batch loss for Iteration: 1 - Batch: 98 is 0.10994724747708642\n",
      "The batch loss for Iteration: 1 - Batch: 99 is 0.10807273907633336\n",
      "Iteration: 2\n",
      "The batch loss for Iteration: 2 - Batch: 0 is 9.306741714477539\n",
      "The batch loss for Iteration: 2 - Batch: 1 is 9.432140350341797\n",
      "The batch loss for Iteration: 2 - Batch: 2 is 6.388742764790853\n",
      "The batch loss for Iteration: 2 - Batch: 3 is 4.080795129140218\n",
      "The batch loss for Iteration: 2 - Batch: 4 is 2.835388151804606\n",
      "The batch loss for Iteration: 2 - Batch: 5 is 2.1913158045874703\n",
      "The batch loss for Iteration: 2 - Batch: 6 is 1.8073413250938295\n",
      "The batch loss for Iteration: 2 - Batch: 7 is 1.5469335004450784\n",
      "The batch loss for Iteration: 2 - Batch: 8 is 1.3459393545958942\n",
      "The batch loss for Iteration: 2 - Batch: 9 is 1.1678621010479682\n",
      "The batch loss for Iteration: 2 - Batch: 10 is 1.0278812036053462\n",
      "The batch loss for Iteration: 2 - Batch: 11 is 0.9085185973605855\n",
      "The batch loss for Iteration: 2 - Batch: 12 is 0.7994620100715324\n",
      "The batch loss for Iteration: 2 - Batch: 13 is 0.7103778073905378\n",
      "The batch loss for Iteration: 2 - Batch: 14 is 0.6323359247692325\n",
      "The batch loss for Iteration: 2 - Batch: 15 is 0.5579476520210995\n",
      "The batch loss for Iteration: 2 - Batch: 16 is 0.5305863838849727\n",
      "The batch loss for Iteration: 2 - Batch: 17 is 0.5039552131265306\n",
      "The batch loss for Iteration: 2 - Batch: 18 is 0.4836140854262987\n",
      "The batch loss for Iteration: 2 - Batch: 19 is 0.4710613786334975\n",
      "The batch loss for Iteration: 2 - Batch: 20 is 0.4588524344381604\n",
      "The batch loss for Iteration: 2 - Batch: 21 is 0.4495809576855325\n",
      "The batch loss for Iteration: 2 - Batch: 22 is 0.44380966693571605\n",
      "The batch loss for Iteration: 2 - Batch: 23 is 0.4254182642824167\n",
      "The batch loss for Iteration: 2 - Batch: 24 is 0.40657292869141387\n",
      "The batch loss for Iteration: 2 - Batch: 25 is 0.3890998772575499\n",
      "The batch loss for Iteration: 2 - Batch: 26 is 0.3761819836968038\n",
      "The batch loss for Iteration: 2 - Batch: 27 is 0.3633915832974863\n",
      "The batch loss for Iteration: 2 - Batch: 28 is 0.3539465770342513\n",
      "The batch loss for Iteration: 2 - Batch: 29 is 0.3455128951377954\n",
      "The batch loss for Iteration: 2 - Batch: 30 is 0.33656891021519353\n",
      "The batch loss for Iteration: 2 - Batch: 31 is 0.32893657927506587\n",
      "The batch loss for Iteration: 2 - Batch: 32 is 0.32205526763886916\n",
      "The batch loss for Iteration: 2 - Batch: 33 is 0.312791078913063\n",
      "The batch loss for Iteration: 2 - Batch: 34 is 0.30492300901037883\n",
      "The batch loss for Iteration: 2 - Batch: 35 is 0.29934689598867803\n",
      "The batch loss for Iteration: 2 - Batch: 36 is 0.2906412684159208\n",
      "The batch loss for Iteration: 2 - Batch: 37 is 0.2841775406955542\n",
      "The batch loss for Iteration: 2 - Batch: 38 is 0.27842721569629525\n",
      "The batch loss for Iteration: 2 - Batch: 39 is 0.27252733936610124\n",
      "The batch loss for Iteration: 2 - Batch: 40 is 0.2665758046267242\n",
      "The batch loss for Iteration: 2 - Batch: 41 is 0.26172535739427005\n",
      "The batch loss for Iteration: 2 - Batch: 42 is 0.2535354687081948\n",
      "The batch loss for Iteration: 2 - Batch: 43 is 0.2474961759315006\n",
      "The batch loss for Iteration: 2 - Batch: 44 is 0.23907899962898993\n",
      "The batch loss for Iteration: 2 - Batch: 45 is 0.2317965860183336\n",
      "The batch loss for Iteration: 2 - Batch: 46 is 0.22222539895607077\n",
      "The batch loss for Iteration: 2 - Batch: 47 is 0.21268320520591263\n",
      "The batch loss for Iteration: 2 - Batch: 48 is 0.20424012757480378\n",
      "The batch loss for Iteration: 2 - Batch: 49 is 0.19931894866966016\n",
      "The batch loss for Iteration: 2 - Batch: 50 is 0.19341792435590402\n",
      "The batch loss for Iteration: 2 - Batch: 51 is 0.18977728978394393\n",
      "The batch loss for Iteration: 2 - Batch: 52 is 0.18602345865020192\n",
      "The batch loss for Iteration: 2 - Batch: 53 is 0.18348103512971659\n",
      "The batch loss for Iteration: 2 - Batch: 54 is 0.18235657102224484\n",
      "The batch loss for Iteration: 2 - Batch: 55 is 0.180580049307282\n",
      "The batch loss for Iteration: 2 - Batch: 56 is 0.1760618461773259\n",
      "The batch loss for Iteration: 2 - Batch: 57 is 0.1742984874728176\n",
      "The batch loss for Iteration: 2 - Batch: 58 is 0.17177991076324425\n",
      "The batch loss for Iteration: 2 - Batch: 59 is 0.1708131200744069\n",
      "The batch loss for Iteration: 2 - Batch: 60 is 0.16982359242353504\n",
      "The batch loss for Iteration: 2 - Batch: 61 is 0.16805254941215977\n",
      "The batch loss for Iteration: 2 - Batch: 62 is 0.16502430128752094\n",
      "The batch loss for Iteration: 2 - Batch: 63 is 0.162531547572417\n",
      "The batch loss for Iteration: 2 - Batch: 64 is 0.1581063590420546\n",
      "The batch loss for Iteration: 2 - Batch: 65 is 0.15511947015616218\n",
      "The batch loss for Iteration: 2 - Batch: 66 is 0.1545071751827762\n",
      "The batch loss for Iteration: 2 - Batch: 67 is 0.1545745136558933\n",
      "The batch loss for Iteration: 2 - Batch: 68 is 0.15449365153148542\n",
      "The batch loss for Iteration: 2 - Batch: 69 is 0.1552409898914375\n",
      "The batch loss for Iteration: 2 - Batch: 70 is 0.15593880634867177\n",
      "The batch loss for Iteration: 2 - Batch: 71 is 0.1579992862516756\n",
      "The batch loss for Iteration: 2 - Batch: 72 is 0.15737083199358165\n",
      "The batch loss for Iteration: 2 - Batch: 73 is 0.15280502264600834\n",
      "The batch loss for Iteration: 2 - Batch: 74 is 0.14908614704673845\n",
      "The batch loss for Iteration: 2 - Batch: 75 is 0.14536980774183875\n",
      "The batch loss for Iteration: 2 - Batch: 76 is 0.14105171713299822\n",
      "The batch loss for Iteration: 2 - Batch: 77 is 0.13671226312373785\n",
      "The batch loss for Iteration: 2 - Batch: 78 is 0.1322978683425473\n",
      "The batch loss for Iteration: 2 - Batch: 79 is 0.1303149744223971\n",
      "The batch loss for Iteration: 2 - Batch: 80 is 0.12723316563052825\n",
      "The batch loss for Iteration: 2 - Batch: 81 is 0.12460194150278274\n",
      "The batch loss for Iteration: 2 - Batch: 82 is 0.12303740266332537\n",
      "The batch loss for Iteration: 2 - Batch: 83 is 0.12174299861743448\n",
      "The batch loss for Iteration: 2 - Batch: 84 is 0.12149494156016491\n",
      "The batch loss for Iteration: 2 - Batch: 85 is 0.12107610347483204\n",
      "The batch loss for Iteration: 2 - Batch: 86 is 0.11924591827696467\n",
      "The batch loss for Iteration: 2 - Batch: 87 is 0.12010729755058275\n",
      "The batch loss for Iteration: 2 - Batch: 88 is 0.12004317197517222\n",
      "The batch loss for Iteration: 2 - Batch: 89 is 0.11927700455026667\n",
      "The batch loss for Iteration: 2 - Batch: 90 is 0.11816618997262912\n",
      "The batch loss for Iteration: 2 - Batch: 91 is 0.11759115541309625\n",
      "The batch loss for Iteration: 2 - Batch: 92 is 0.11690327699085402\n",
      "The batch loss for Iteration: 2 - Batch: 93 is 0.11632832030614855\n",
      "The batch loss for Iteration: 2 - Batch: 94 is 0.11458769431659639\n",
      "The batch loss for Iteration: 2 - Batch: 95 is 0.1130948852117431\n",
      "The batch loss for Iteration: 2 - Batch: 96 is 0.11126915039333646\n",
      "The batch loss for Iteration: 2 - Batch: 97 is 0.11079632662406108\n",
      "The batch loss for Iteration: 2 - Batch: 98 is 0.1099474393380883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:23<06:08,  7.85s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 2 - Batch: 99 is 0.10807281728888869\n",
      "Iteration: 3\n",
      "The batch loss for Iteration: 3 - Batch: 0 is 9.306758880615234\n",
      "The batch loss for Iteration: 3 - Batch: 1 is 9.43214225769043\n",
      "The batch loss for Iteration: 3 - Batch: 2 is 6.388739585876465\n",
      "The batch loss for Iteration: 3 - Batch: 3 is 4.080795526504517\n",
      "The batch loss for Iteration: 3 - Batch: 4 is 2.8353882312774656\n",
      "The batch loss for Iteration: 3 - Batch: 5 is 2.19131502310435\n",
      "The batch loss for Iteration: 3 - Batch: 6 is 1.8073335840588525\n",
      "The batch loss for Iteration: 3 - Batch: 7 is 1.5469382548616046\n",
      "The batch loss for Iteration: 3 - Batch: 8 is 1.345938399371016\n",
      "The batch loss for Iteration: 3 - Batch: 9 is 1.167866392427336\n",
      "The batch loss for Iteration: 3 - Batch: 10 is 1.0278851483350133\n",
      "The batch loss for Iteration: 3 - Batch: 11 is 0.9085231381496218\n",
      "The batch loss for Iteration: 3 - Batch: 12 is 0.7994640466329412\n",
      "The batch loss for Iteration: 3 - Batch: 13 is 0.7103774760220516\n",
      "The batch loss for Iteration: 3 - Batch: 14 is 0.6323325330287487\n",
      "The batch loss for Iteration: 3 - Batch: 15 is 0.5579473804326744\n",
      "The batch loss for Iteration: 3 - Batch: 16 is 0.5305869849925643\n",
      "The batch loss for Iteration: 3 - Batch: 17 is 0.5039587433272236\n",
      "The batch loss for Iteration: 3 - Batch: 18 is 0.4836195415317679\n",
      "The batch loss for Iteration: 3 - Batch: 19 is 0.47106360647111967\n",
      "The batch loss for Iteration: 3 - Batch: 20 is 0.45885640063599453\n",
      "The batch loss for Iteration: 3 - Batch: 21 is 0.44958850726878813\n",
      "The batch loss for Iteration: 3 - Batch: 22 is 0.443810326891272\n",
      "The batch loss for Iteration: 3 - Batch: 23 is 0.42541864940843355\n",
      "The batch loss for Iteration: 3 - Batch: 24 is 0.40657397406471624\n",
      "The batch loss for Iteration: 3 - Batch: 25 is 0.38910017422268517\n",
      "The batch loss for Iteration: 3 - Batch: 26 is 0.3761821713018674\n",
      "The batch loss for Iteration: 3 - Batch: 27 is 0.3633922711936074\n",
      "The batch loss for Iteration: 3 - Batch: 28 is 0.3539458443924183\n",
      "The batch loss for Iteration: 3 - Batch: 29 is 0.3455108362111926\n",
      "The batch loss for Iteration: 3 - Batch: 30 is 0.33656656728532197\n",
      "The batch loss for Iteration: 3 - Batch: 31 is 0.3289352841632895\n",
      "The batch loss for Iteration: 3 - Batch: 32 is 0.322055806377492\n",
      "The batch loss for Iteration: 3 - Batch: 33 is 0.31279176794018704\n",
      "The batch loss for Iteration: 3 - Batch: 34 is 0.3049236281492955\n",
      "The batch loss for Iteration: 3 - Batch: 35 is 0.299346542313636\n",
      "The batch loss for Iteration: 3 - Batch: 36 is 0.2906406918075423\n",
      "The batch loss for Iteration: 3 - Batch: 37 is 0.28417717416795396\n",
      "The batch loss for Iteration: 3 - Batch: 38 is 0.2784271573917765\n",
      "The batch loss for Iteration: 3 - Batch: 39 is 0.272527933954936\n",
      "The batch loss for Iteration: 3 - Batch: 40 is 0.26657598195133514\n",
      "The batch loss for Iteration: 3 - Batch: 41 is 0.26172577033384875\n",
      "The batch loss for Iteration: 3 - Batch: 42 is 0.25353567791769305\n",
      "The batch loss for Iteration: 3 - Batch: 43 is 0.24749637575600847\n",
      "The batch loss for Iteration: 3 - Batch: 44 is 0.23907968223793732\n",
      "The batch loss for Iteration: 3 - Batch: 45 is 0.23179670451791032\n",
      "The batch loss for Iteration: 3 - Batch: 46 is 0.2222252391497951\n",
      "The batch loss for Iteration: 3 - Batch: 47 is 0.21268300319446595\n",
      "The batch loss for Iteration: 3 - Batch: 48 is 0.20423846911912333\n",
      "The batch loss for Iteration: 3 - Batch: 49 is 0.19931941141119108\n",
      "The batch loss for Iteration: 3 - Batch: 50 is 0.19341669926250496\n",
      "The batch loss for Iteration: 3 - Batch: 51 is 0.189781392699863\n",
      "The batch loss for Iteration: 3 - Batch: 52 is 0.18602490359669185\n",
      "The batch loss for Iteration: 3 - Batch: 53 is 0.18348005523176203\n",
      "The batch loss for Iteration: 3 - Batch: 54 is 0.18235516604327634\n",
      "The batch loss for Iteration: 3 - Batch: 55 is 0.18057988797918378\n",
      "The batch loss for Iteration: 3 - Batch: 56 is 0.17606182661588\n",
      "The batch loss for Iteration: 3 - Batch: 57 is 0.1742988488740851\n",
      "The batch loss for Iteration: 3 - Batch: 58 is 0.17177849445920057\n",
      "The batch loss for Iteration: 3 - Batch: 59 is 0.1708137958305049\n",
      "The batch loss for Iteration: 3 - Batch: 60 is 0.16982308757933268\n",
      "The batch loss for Iteration: 3 - Batch: 61 is 0.1680504647205966\n",
      "The batch loss for Iteration: 3 - Batch: 62 is 0.16502488884236874\n",
      "The batch loss for Iteration: 3 - Batch: 63 is 0.16253005173568094\n",
      "The batch loss for Iteration: 3 - Batch: 64 is 0.15810774453278753\n",
      "The batch loss for Iteration: 3 - Batch: 65 is 0.15511952004766769\n",
      "The batch loss for Iteration: 3 - Batch: 66 is 0.15450780222100646\n",
      "The batch loss for Iteration: 3 - Batch: 67 is 0.15457413018761934\n",
      "The batch loss for Iteration: 3 - Batch: 68 is 0.1544926784782909\n",
      "The batch loss for Iteration: 3 - Batch: 69 is 0.15523972259014748\n",
      "The batch loss for Iteration: 3 - Batch: 70 is 0.1559377139367478\n",
      "The batch loss for Iteration: 3 - Batch: 71 is 0.15799935055214748\n",
      "The batch loss for Iteration: 3 - Batch: 72 is 0.15737095045069563\n",
      "The batch loss for Iteration: 3 - Batch: 73 is 0.15280556552139216\n",
      "The batch loss for Iteration: 3 - Batch: 74 is 0.14908728597859902\n",
      "The batch loss for Iteration: 3 - Batch: 75 is 0.14537112775579622\n",
      "The batch loss for Iteration: 3 - Batch: 76 is 0.14105292327258695\n",
      "The batch loss for Iteration: 3 - Batch: 77 is 0.1367126209316923\n",
      "The batch loss for Iteration: 3 - Batch: 78 is 0.13229799359002983\n",
      "The batch loss for Iteration: 3 - Batch: 79 is 0.13031501175077748\n",
      "The batch loss for Iteration: 3 - Batch: 80 is 0.12723320141264344\n",
      "The batch loss for Iteration: 3 - Batch: 81 is 0.12460219780299098\n",
      "The batch loss for Iteration: 3 - Batch: 82 is 0.12303737128112367\n",
      "The batch loss for Iteration: 3 - Batch: 83 is 0.12174235110769357\n",
      "The batch loss for Iteration: 3 - Batch: 84 is 0.12149467588935292\n",
      "The batch loss for Iteration: 3 - Batch: 85 is 0.12107652177661357\n",
      "The batch loss for Iteration: 3 - Batch: 86 is 0.11924568192600858\n",
      "The batch loss for Iteration: 3 - Batch: 87 is 0.12010680719041007\n",
      "The batch loss for Iteration: 3 - Batch: 88 is 0.12004314503462386\n",
      "The batch loss for Iteration: 3 - Batch: 89 is 0.11927667576310715\n",
      "The batch loss for Iteration: 3 - Batch: 90 is 0.11816611300002063\n",
      "The batch loss for Iteration: 3 - Batch: 91 is 0.11759127896873961\n",
      "The batch loss for Iteration: 3 - Batch: 92 is 0.11690303220990832\n",
      "The batch loss for Iteration: 3 - Batch: 93 is 0.11632832784756739\n",
      "The batch loss for Iteration: 3 - Batch: 94 is 0.1145878048214269\n",
      "The batch loss for Iteration: 3 - Batch: 95 is 0.11309498570390973\n",
      "The batch loss for Iteration: 3 - Batch: 96 is 0.11126945621185168\n",
      "The batch loss for Iteration: 3 - Batch: 97 is 0.1107964367897345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:31<06:00,  7.83s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 3 - Batch: 98 is 0.10994709366021235\n",
      "The batch loss for Iteration: 3 - Batch: 99 is 0.108072584950274\n",
      "Iteration: 4\n",
      "The batch loss for Iteration: 4 - Batch: 0 is 9.306719779968262\n",
      "The batch loss for Iteration: 4 - Batch: 1 is 9.432099342346191\n",
      "The batch loss for Iteration: 4 - Batch: 2 is 6.388711611429851\n",
      "The batch loss for Iteration: 4 - Batch: 3 is 4.080775658289592\n",
      "The batch loss for Iteration: 4 - Batch: 4 is 2.835374720891317\n",
      "The batch loss for Iteration: 4 - Batch: 5 is 2.1913043472501967\n",
      "The batch loss for Iteration: 4 - Batch: 6 is 1.807335328677344\n",
      "The batch loss for Iteration: 4 - Batch: 7 is 1.5469369232181518\n",
      "The batch loss for Iteration: 4 - Batch: 8 is 1.3459348605686183\n",
      "The batch loss for Iteration: 4 - Batch: 9 is 1.1678812973361588\n",
      "The batch loss for Iteration: 4 - Batch: 10 is 1.0279106052776301\n",
      "The batch loss for Iteration: 4 - Batch: 11 is 0.9085487040551182\n",
      "The batch loss for Iteration: 4 - Batch: 12 is 0.7994826658618119\n",
      "The batch loss for Iteration: 4 - Batch: 13 is 0.7103868440790663\n",
      "The batch loss for Iteration: 4 - Batch: 14 is 0.632340087599249\n",
      "The batch loss for Iteration: 4 - Batch: 15 is 0.5579473161515278\n",
      "The batch loss for Iteration: 4 - Batch: 16 is 0.530586027537004\n",
      "The batch loss for Iteration: 4 - Batch: 17 is 0.5039558291122987\n",
      "The batch loss for Iteration: 4 - Batch: 18 is 0.4836142684267575\n",
      "The batch loss for Iteration: 4 - Batch: 19 is 0.4710595281186035\n",
      "The batch loss for Iteration: 4 - Batch: 20 is 0.45884939446932893\n",
      "The batch loss for Iteration: 4 - Batch: 21 is 0.44957977913314956\n",
      "The batch loss for Iteration: 4 - Batch: 22 is 0.443808910804596\n",
      "The batch loss for Iteration: 4 - Batch: 23 is 0.42541882882340115\n",
      "The batch loss for Iteration: 4 - Batch: 24 is 0.4065767659703189\n",
      "The batch loss for Iteration: 4 - Batch: 25 is 0.3891055634921915\n",
      "The batch loss for Iteration: 4 - Batch: 26 is 0.3761861149591654\n",
      "The batch loss for Iteration: 4 - Batch: 27 is 0.3633945918655198\n",
      "The batch loss for Iteration: 4 - Batch: 28 is 0.3539456942183389\n",
      "The batch loss for Iteration: 4 - Batch: 29 is 0.34550755692357027\n",
      "The batch loss for Iteration: 4 - Batch: 30 is 0.3365636312425887\n",
      "The batch loss for Iteration: 4 - Batch: 31 is 0.32893575865607944\n",
      "The batch loss for Iteration: 4 - Batch: 32 is 0.32205767030625077\n",
      "The batch loss for Iteration: 4 - Batch: 33 is 0.3127939825534553\n",
      "The batch loss for Iteration: 4 - Batch: 34 is 0.30492565326826837\n",
      "The batch loss for Iteration: 4 - Batch: 35 is 0.2993467045307538\n",
      "The batch loss for Iteration: 4 - Batch: 36 is 0.29064002604226913\n",
      "The batch loss for Iteration: 4 - Batch: 37 is 0.28417660452057936\n",
      "The batch loss for Iteration: 4 - Batch: 38 is 0.27842699606630794\n",
      "The batch loss for Iteration: 4 - Batch: 39 is 0.27252804913108886\n",
      "The batch loss for Iteration: 4 - Batch: 40 is 0.2665761243226047\n",
      "The batch loss for Iteration: 4 - Batch: 41 is 0.26172572831057817\n",
      "The batch loss for Iteration: 4 - Batch: 42 is 0.25353563258346273\n",
      "The batch loss for Iteration: 4 - Batch: 43 is 0.24749624467918735\n",
      "The batch loss for Iteration: 4 - Batch: 44 is 0.23907942501196805\n",
      "The batch loss for Iteration: 4 - Batch: 45 is 0.2317966574619407\n",
      "The batch loss for Iteration: 4 - Batch: 46 is 0.2222255628036907\n",
      "The batch loss for Iteration: 4 - Batch: 47 is 0.21268424176658082\n",
      "The batch loss for Iteration: 4 - Batch: 48 is 0.2042403238937735\n",
      "The batch loss for Iteration: 4 - Batch: 49 is 0.19932066920980904\n",
      "The batch loss for Iteration: 4 - Batch: 50 is 0.19341567675342458\n",
      "The batch loss for Iteration: 4 - Batch: 51 is 0.18977821857502641\n",
      "The batch loss for Iteration: 4 - Batch: 52 is 0.1860207231147429\n",
      "The batch loss for Iteration: 4 - Batch: 53 is 0.18347817643060974\n",
      "The batch loss for Iteration: 4 - Batch: 54 is 0.18235488912979306\n",
      "The batch loss for Iteration: 4 - Batch: 55 is 0.18058003630338673\n",
      "The batch loss for Iteration: 4 - Batch: 56 is 0.17606172883128884\n",
      "The batch loss for Iteration: 4 - Batch: 57 is 0.17429878141750135\n",
      "The batch loss for Iteration: 4 - Batch: 58 is 0.17177886508721235\n",
      "The batch loss for Iteration: 4 - Batch: 59 is 0.17081399274250172\n",
      "The batch loss for Iteration: 4 - Batch: 60 is 0.1698232315134449\n",
      "The batch loss for Iteration: 4 - Batch: 61 is 0.16805099002480417\n",
      "The batch loss for Iteration: 4 - Batch: 62 is 0.16502462470215462\n",
      "The batch loss for Iteration: 4 - Batch: 63 is 0.1625302413235856\n",
      "The batch loss for Iteration: 4 - Batch: 64 is 0.1581067204156453\n",
      "The batch loss for Iteration: 4 - Batch: 65 is 0.1551189987943614\n",
      "The batch loss for Iteration: 4 - Batch: 66 is 0.15450648491816443\n",
      "The batch loss for Iteration: 4 - Batch: 67 is 0.15457384434769503\n",
      "The batch loss for Iteration: 4 - Batch: 68 is 0.15449337922539544\n",
      "The batch loss for Iteration: 4 - Batch: 69 is 0.1552409860013505\n",
      "The batch loss for Iteration: 4 - Batch: 70 is 0.1559385376532293\n",
      "The batch loss for Iteration: 4 - Batch: 71 is 0.15799834209095467\n",
      "The batch loss for Iteration: 4 - Batch: 72 is 0.15737143306936452\n",
      "The batch loss for Iteration: 4 - Batch: 73 is 0.15280598444297047\n",
      "The batch loss for Iteration: 4 - Batch: 74 is 0.14908745686776825\n",
      "The batch loss for Iteration: 4 - Batch: 75 is 0.14537105471426032\n",
      "The batch loss for Iteration: 4 - Batch: 76 is 0.1410530461778029\n",
      "The batch loss for Iteration: 4 - Batch: 77 is 0.13671338055621582\n",
      "The batch loss for Iteration: 4 - Batch: 78 is 0.1322980514928373\n",
      "The batch loss for Iteration: 4 - Batch: 79 is 0.1303150005536336\n",
      "The batch loss for Iteration: 4 - Batch: 80 is 0.12723335433324798\n",
      "The batch loss for Iteration: 4 - Batch: 81 is 0.12460210662647968\n",
      "The batch loss for Iteration: 4 - Batch: 82 is 0.12303730124229945\n",
      "The batch loss for Iteration: 4 - Batch: 83 is 0.12174267951860251\n",
      "The batch loss for Iteration: 4 - Batch: 84 is 0.12149456755603227\n",
      "The batch loss for Iteration: 4 - Batch: 85 is 0.12107605476900184\n",
      "The batch loss for Iteration: 4 - Batch: 86 is 0.1192462904174351\n",
      "The batch loss for Iteration: 4 - Batch: 87 is 0.12010750768640639\n",
      "The batch loss for Iteration: 4 - Batch: 88 is 0.12004327077522471\n",
      "The batch loss for Iteration: 4 - Batch: 89 is 0.11927711161185797\n",
      "The batch loss for Iteration: 4 - Batch: 90 is 0.11816619114913013\n",
      "The batch loss for Iteration: 4 - Batch: 91 is 0.11759113469383396\n",
      "The batch loss for Iteration: 4 - Batch: 92 is 0.11690364593231777\n",
      "The batch loss for Iteration: 4 - Batch: 93 is 0.11632826335822903\n",
      "The batch loss for Iteration: 4 - Batch: 94 is 0.11458770375582161\n",
      "The batch loss for Iteration: 4 - Batch: 95 is 0.11309477603488628\n",
      "The batch loss for Iteration: 4 - Batch: 96 is 0.11126939506015146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:39<05:52,  7.82s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 4 - Batch: 97 is 0.11079636804614353\n",
      "The batch loss for Iteration: 4 - Batch: 98 is 0.10994729526038462\n",
      "The batch loss for Iteration: 4 - Batch: 99 is 0.10807273955416635\n",
      "Iteration: 5\n",
      "The batch loss for Iteration: 5 - Batch: 0 is 9.306743621826172\n",
      "The batch loss for Iteration: 5 - Batch: 1 is 9.432146072387695\n",
      "The batch loss for Iteration: 5 - Batch: 2 is 6.388743082682292\n",
      "The batch loss for Iteration: 5 - Batch: 3 is 4.080793301264445\n",
      "The batch loss for Iteration: 5 - Batch: 4 is 2.835385306676229\n",
      "The batch loss for Iteration: 5 - Batch: 5 is 2.191314853562249\n",
      "The batch loss for Iteration: 5 - Batch: 6 is 1.8073364208615015\n",
      "The batch loss for Iteration: 5 - Batch: 7 is 1.546935867648276\n",
      "The batch loss for Iteration: 5 - Batch: 8 is 1.3459377102698389\n",
      "The batch loss for Iteration: 5 - Batch: 9 is 1.1678598385318666\n",
      "The batch loss for Iteration: 5 - Batch: 10 is 1.027882124991711\n",
      "The batch loss for Iteration: 5 - Batch: 11 is 0.9085250319715586\n",
      "The batch loss for Iteration: 5 - Batch: 12 is 0.7994683738066313\n",
      "The batch loss for Iteration: 5 - Batch: 13 is 0.7103809867268059\n",
      "The batch loss for Iteration: 5 - Batch: 14 is 0.6323403964702636\n",
      "The batch loss for Iteration: 5 - Batch: 15 is 0.5579474546652557\n",
      "The batch loss for Iteration: 5 - Batch: 16 is 0.5305855307984675\n",
      "The batch loss for Iteration: 5 - Batch: 17 is 0.5039542120585194\n",
      "The batch loss for Iteration: 5 - Batch: 18 is 0.48361348061127285\n",
      "The batch loss for Iteration: 5 - Batch: 19 is 0.47106120534159884\n",
      "The batch loss for Iteration: 5 - Batch: 20 is 0.45885242618616523\n",
      "The batch loss for Iteration: 5 - Batch: 21 is 0.4495818242870931\n",
      "The batch loss for Iteration: 5 - Batch: 22 is 0.4438093314371384\n",
      "The batch loss for Iteration: 5 - Batch: 23 is 0.4254184489854586\n",
      "The batch loss for Iteration: 5 - Batch: 24 is 0.40657423307660584\n",
      "The batch loss for Iteration: 5 - Batch: 25 is 0.3891005876622763\n",
      "The batch loss for Iteration: 5 - Batch: 26 is 0.3761814095464833\n",
      "The batch loss for Iteration: 5 - Batch: 27 is 0.3633917330911028\n",
      "The batch loss for Iteration: 5 - Batch: 28 is 0.35394506947477067\n",
      "The batch loss for Iteration: 5 - Batch: 29 is 0.34550966597142463\n",
      "The batch loss for Iteration: 5 - Batch: 30 is 0.3365660065529624\n",
      "The batch loss for Iteration: 5 - Batch: 31 is 0.32893639912865397\n",
      "The batch loss for Iteration: 5 - Batch: 32 is 0.3220558979627646\n",
      "The batch loss for Iteration: 5 - Batch: 33 is 0.31279224747102974\n",
      "The batch loss for Iteration: 5 - Batch: 34 is 0.30492451378098034\n",
      "The batch loss for Iteration: 5 - Batch: 35 is 0.2993458251678256\n",
      "The batch loss for Iteration: 5 - Batch: 36 is 0.2906401053756296\n",
      "The batch loss for Iteration: 5 - Batch: 37 is 0.2841767571884546\n",
      "The batch loss for Iteration: 5 - Batch: 38 is 0.278427048887244\n",
      "The batch loss for Iteration: 5 - Batch: 39 is 0.2725279789260385\n",
      "The batch loss for Iteration: 5 - Batch: 40 is 0.2665767273793651\n",
      "The batch loss for Iteration: 5 - Batch: 41 is 0.261726174093168\n",
      "The batch loss for Iteration: 5 - Batch: 42 is 0.25353590909216944\n",
      "The batch loss for Iteration: 5 - Batch: 43 is 0.2474964460332227\n",
      "The batch loss for Iteration: 5 - Batch: 44 is 0.23907981095622874\n",
      "The batch loss for Iteration: 5 - Batch: 45 is 0.2317971634212419\n",
      "The batch loss for Iteration: 5 - Batch: 46 is 0.22222658811592735\n",
      "The batch loss for Iteration: 5 - Batch: 47 is 0.21268525653799863\n",
      "The batch loss for Iteration: 5 - Batch: 48 is 0.2042411620385226\n",
      "The batch loss for Iteration: 5 - Batch: 49 is 0.19932097207499896\n",
      "The batch loss for Iteration: 5 - Batch: 50 is 0.19341583228792888\n",
      "The batch loss for Iteration: 5 - Batch: 51 is 0.18977787310815128\n",
      "The batch loss for Iteration: 5 - Batch: 52 is 0.18601976292218356\n",
      "The batch loss for Iteration: 5 - Batch: 53 is 0.183477469884482\n",
      "The batch loss for Iteration: 5 - Batch: 54 is 0.18235489362303284\n",
      "The batch loss for Iteration: 5 - Batch: 55 is 0.18058029183210075\n",
      "The batch loss for Iteration: 5 - Batch: 56 is 0.1760616663897353\n",
      "The batch loss for Iteration: 5 - Batch: 57 is 0.17429843504504966\n",
      "The batch loss for Iteration: 5 - Batch: 58 is 0.1717784066252918\n",
      "The batch loss for Iteration: 5 - Batch: 59 is 0.17081331752944823\n",
      "The batch loss for Iteration: 5 - Batch: 60 is 0.16982298593430065\n",
      "The batch loss for Iteration: 5 - Batch: 61 is 0.16805077071803687\n",
      "The batch loss for Iteration: 5 - Batch: 62 is 0.1650244698442192\n",
      "The batch loss for Iteration: 5 - Batch: 63 is 0.162530134595802\n",
      "The batch loss for Iteration: 5 - Batch: 64 is 0.1581068214770673\n",
      "The batch loss for Iteration: 5 - Batch: 65 is 0.1551189569767625\n",
      "The batch loss for Iteration: 5 - Batch: 66 is 0.15450695401420686\n",
      "The batch loss for Iteration: 5 - Batch: 67 is 0.15457396344314467\n",
      "The batch loss for Iteration: 5 - Batch: 68 is 0.1544931736309129\n",
      "The batch loss for Iteration: 5 - Batch: 69 is 0.1552407923294232\n",
      "The batch loss for Iteration: 5 - Batch: 70 is 0.15593877670204292\n",
      "The batch loss for Iteration: 5 - Batch: 71 is 0.15799936531277659\n",
      "The batch loss for Iteration: 5 - Batch: 72 is 0.15737075469242004\n",
      "The batch loss for Iteration: 5 - Batch: 73 is 0.15280508603885187\n",
      "The batch loss for Iteration: 5 - Batch: 74 is 0.14908654207736047\n",
      "The batch loss for Iteration: 5 - Batch: 75 is 0.14537030232511458\n",
      "The batch loss for Iteration: 5 - Batch: 76 is 0.14105219420062548\n",
      "The batch loss for Iteration: 5 - Batch: 77 is 0.13671224478680188\n",
      "The batch loss for Iteration: 5 - Batch: 78 is 0.13229798882870208\n",
      "The batch loss for Iteration: 5 - Batch: 79 is 0.13031504745404773\n",
      "The batch loss for Iteration: 5 - Batch: 80 is 0.12723328426972347\n",
      "The batch loss for Iteration: 5 - Batch: 81 is 0.12460224533414153\n",
      "The batch loss for Iteration: 5 - Batch: 82 is 0.12303737185378813\n",
      "The batch loss for Iteration: 5 - Batch: 83 is 0.12174236246777669\n",
      "The batch loss for Iteration: 5 - Batch: 84 is 0.1214945526063247\n",
      "The batch loss for Iteration: 5 - Batch: 85 is 0.12107623202294783\n",
      "The batch loss for Iteration: 5 - Batch: 86 is 0.11924552513067414\n",
      "The batch loss for Iteration: 5 - Batch: 87 is 0.12010676205981236\n",
      "The batch loss for Iteration: 5 - Batch: 88 is 0.12004311238121322\n",
      "The batch loss for Iteration: 5 - Batch: 89 is 0.11927670718943535\n",
      "The batch loss for Iteration: 5 - Batch: 90 is 0.11816609238548981\n",
      "The batch loss for Iteration: 5 - Batch: 91 is 0.11759125801261827\n",
      "The batch loss for Iteration: 5 - Batch: 92 is 0.1169032063121369\n",
      "The batch loss for Iteration: 5 - Batch: 93 is 0.11632829926330439\n",
      "The batch loss for Iteration: 5 - Batch: 94 is 0.11458772421112381\n",
      "The batch loss for Iteration: 5 - Batch: 95 is 0.11309480605028473\n",
      "The batch loss for Iteration: 5 - Batch: 96 is 0.11126931671603668\n",
      "The batch loss for Iteration: 5 - Batch: 97 is 0.11079630885849033\n",
      "The batch loss for Iteration: 5 - Batch: 98 is 0.10994716943256878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:46<05:43,  7.81s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 5 - Batch: 99 is 0.10807277644286085\n",
      "Iteration: 6\n",
      "The batch loss for Iteration: 6 - Batch: 0 is 9.306737899780273\n",
      "The batch loss for Iteration: 6 - Batch: 1 is 9.432142734527588\n",
      "The batch loss for Iteration: 6 - Batch: 2 is 6.388745466868083\n",
      "The batch loss for Iteration: 6 - Batch: 3 is 4.0807972351710005\n",
      "The batch loss for Iteration: 6 - Batch: 4 is 2.835389335950216\n",
      "The batch loss for Iteration: 6 - Batch: 5 is 2.1913177503479853\n",
      "The batch loss for Iteration: 6 - Batch: 6 is 1.8073422842555575\n",
      "The batch loss for Iteration: 6 - Batch: 7 is 1.5469324282473988\n",
      "The batch loss for Iteration: 6 - Batch: 8 is 1.3459400831733221\n",
      "The batch loss for Iteration: 6 - Batch: 9 is 1.1678604572919415\n",
      "The batch loss for Iteration: 6 - Batch: 10 is 1.0278794069173427\n",
      "The batch loss for Iteration: 6 - Batch: 11 is 0.9085168581793912\n",
      "The batch loss for Iteration: 6 - Batch: 12 is 0.7994602623779821\n",
      "The batch loss for Iteration: 6 - Batch: 13 is 0.710374957771523\n",
      "The batch loss for Iteration: 6 - Batch: 14 is 0.632334526807164\n",
      "The batch loss for Iteration: 6 - Batch: 15 is 0.557947087811312\n",
      "The batch loss for Iteration: 6 - Batch: 16 is 0.5305851165293993\n",
      "The batch loss for Iteration: 6 - Batch: 17 is 0.5039541360616648\n",
      "The batch loss for Iteration: 6 - Batch: 18 is 0.483613878158519\n",
      "The batch loss for Iteration: 6 - Batch: 19 is 0.4710622742607091\n",
      "The batch loss for Iteration: 6 - Batch: 20 is 0.45885297663076485\n",
      "The batch loss for Iteration: 6 - Batch: 21 is 0.4495804187958275\n",
      "The batch loss for Iteration: 6 - Batch: 22 is 0.44380964350572893\n",
      "The batch loss for Iteration: 6 - Batch: 23 is 0.4254183030425971\n",
      "The batch loss for Iteration: 6 - Batch: 24 is 0.4065730828297117\n",
      "The batch loss for Iteration: 6 - Batch: 25 is 0.3890990762307551\n",
      "The batch loss for Iteration: 6 - Batch: 26 is 0.37618121228245416\n",
      "The batch loss for Iteration: 6 - Batch: 27 is 0.36339176010575586\n",
      "The batch loss for Iteration: 6 - Batch: 28 is 0.3539462213925544\n",
      "The batch loss for Iteration: 6 - Batch: 29 is 0.3455126925482089\n",
      "The batch loss for Iteration: 6 - Batch: 30 is 0.3365687190979198\n",
      "The batch loss for Iteration: 6 - Batch: 31 is 0.3289369905351645\n",
      "The batch loss for Iteration: 6 - Batch: 32 is 0.32205475991530563\n",
      "The batch loss for Iteration: 6 - Batch: 33 is 0.31279106398001705\n",
      "The batch loss for Iteration: 6 - Batch: 34 is 0.3049227088575066\n",
      "The batch loss for Iteration: 6 - Batch: 35 is 0.29934577503106247\n",
      "The batch loss for Iteration: 6 - Batch: 36 is 0.2906415474195473\n",
      "The batch loss for Iteration: 6 - Batch: 37 is 0.28417794958483544\n",
      "The batch loss for Iteration: 6 - Batch: 38 is 0.2784274218061366\n",
      "The batch loss for Iteration: 6 - Batch: 39 is 0.27252722530955775\n",
      "The batch loss for Iteration: 6 - Batch: 40 is 0.2665755459810163\n",
      "The batch loss for Iteration: 6 - Batch: 41 is 0.2617249879315374\n",
      "The batch loss for Iteration: 6 - Batch: 42 is 0.2535355044729831\n",
      "The batch loss for Iteration: 6 - Batch: 43 is 0.247496198418753\n",
      "The batch loss for Iteration: 6 - Batch: 44 is 0.23907891535765632\n",
      "The batch loss for Iteration: 6 - Batch: 45 is 0.23179621100944164\n",
      "The batch loss for Iteration: 6 - Batch: 46 is 0.22222537068621528\n",
      "The batch loss for Iteration: 6 - Batch: 47 is 0.2126843172369931\n",
      "The batch loss for Iteration: 6 - Batch: 48 is 0.20424106501814973\n",
      "The batch loss for Iteration: 6 - Batch: 49 is 0.1993210273550505\n",
      "The batch loss for Iteration: 6 - Batch: 50 is 0.1934160016673191\n",
      "The batch loss for Iteration: 6 - Batch: 51 is 0.18977739952828904\n",
      "The batch loss for Iteration: 6 - Batch: 52 is 0.18601939410961393\n",
      "The batch loss for Iteration: 6 - Batch: 53 is 0.1834774277333486\n",
      "The batch loss for Iteration: 6 - Batch: 54 is 0.1823552223077761\n",
      "The batch loss for Iteration: 6 - Batch: 55 is 0.18058034879116672\n",
      "The batch loss for Iteration: 6 - Batch: 56 is 0.17606163392676047\n",
      "The batch loss for Iteration: 6 - Batch: 57 is 0.17429848381332508\n",
      "The batch loss for Iteration: 6 - Batch: 58 is 0.17177826197612958\n",
      "The batch loss for Iteration: 6 - Batch: 59 is 0.1708134581697763\n",
      "The batch loss for Iteration: 6 - Batch: 60 is 0.16982305077590054\n",
      "The batch loss for Iteration: 6 - Batch: 61 is 0.16805074100018147\n",
      "The batch loss for Iteration: 6 - Batch: 62 is 0.16502459047400772\n",
      "The batch loss for Iteration: 6 - Batch: 63 is 0.16253000237019172\n",
      "The batch loss for Iteration: 6 - Batch: 64 is 0.15810671673943924\n",
      "The batch loss for Iteration: 6 - Batch: 65 is 0.15511857969994652\n",
      "The batch loss for Iteration: 6 - Batch: 66 is 0.15450693414926459\n",
      "The batch loss for Iteration: 6 - Batch: 67 is 0.15457420156959223\n",
      "The batch loss for Iteration: 6 - Batch: 68 is 0.1544937990435315\n",
      "The batch loss for Iteration: 6 - Batch: 69 is 0.15524167319469276\n",
      "The batch loss for Iteration: 6 - Batch: 70 is 0.15593980994307557\n",
      "The batch loss for Iteration: 6 - Batch: 71 is 0.15800005518265395\n",
      "The batch loss for Iteration: 6 - Batch: 72 is 0.15737047673399424\n",
      "The batch loss for Iteration: 6 - Batch: 73 is 0.15280434769568346\n",
      "The batch loss for Iteration: 6 - Batch: 74 is 0.1490850953634815\n",
      "The batch loss for Iteration: 6 - Batch: 75 is 0.14536863945604447\n",
      "The batch loss for Iteration: 6 - Batch: 76 is 0.14105057489080877\n",
      "The batch loss for Iteration: 6 - Batch: 77 is 0.1367110258202272\n",
      "The batch loss for Iteration: 6 - Batch: 78 is 0.13229773196220967\n",
      "The batch loss for Iteration: 6 - Batch: 79 is 0.1303148535083533\n",
      "The batch loss for Iteration: 6 - Batch: 80 is 0.12723306994770633\n",
      "The batch loss for Iteration: 6 - Batch: 81 is 0.1246021264187125\n",
      "The batch loss for Iteration: 6 - Batch: 82 is 0.1230373014807601\n",
      "The batch loss for Iteration: 6 - Batch: 83 is 0.1217423389234712\n",
      "The batch loss for Iteration: 6 - Batch: 84 is 0.12149460842782209\n",
      "The batch loss for Iteration: 6 - Batch: 85 is 0.1210763546536336\n",
      "The batch loss for Iteration: 6 - Batch: 86 is 0.119245493654901\n",
      "The batch loss for Iteration: 6 - Batch: 87 is 0.12010667500446798\n",
      "The batch loss for Iteration: 6 - Batch: 88 is 0.12004311140306329\n",
      "The batch loss for Iteration: 6 - Batch: 89 is 0.11927666479304186\n",
      "The batch loss for Iteration: 6 - Batch: 90 is 0.11816612335940799\n",
      "The batch loss for Iteration: 6 - Batch: 91 is 0.11759131017941718\n",
      "The batch loss for Iteration: 6 - Batch: 92 is 0.11690313509113243\n",
      "The batch loss for Iteration: 6 - Batch: 93 is 0.11632831879657704\n",
      "The batch loss for Iteration: 6 - Batch: 94 is 0.11458770433938317\n",
      "The batch loss for Iteration: 6 - Batch: 95 is 0.11309480584328745\n",
      "The batch loss for Iteration: 6 - Batch: 96 is 0.11126927738712676\n",
      "The batch loss for Iteration: 6 - Batch: 97 is 0.11079632791991607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:54<05:35,  7.81s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 6 - Batch: 98 is 0.10994723705662576\n",
      "The batch loss for Iteration: 6 - Batch: 99 is 0.1080727771191014\n",
      "Iteration: 7\n",
      "The batch loss for Iteration: 7 - Batch: 0 is 9.306741714477539\n",
      "The batch loss for Iteration: 7 - Batch: 1 is 9.432146072387695\n",
      "The batch loss for Iteration: 7 - Batch: 2 is 6.388746897379558\n",
      "The batch loss for Iteration: 7 - Batch: 3 is 4.080797592798869\n",
      "The batch loss for Iteration: 7 - Batch: 4 is 2.8353894074757897\n",
      "The batch loss for Iteration: 7 - Batch: 5 is 2.191318239106072\n",
      "The batch loss for Iteration: 7 - Batch: 6 is 1.8073433077524579\n",
      "The batch loss for Iteration: 7 - Batch: 7 is 1.5469321985566427\n",
      "The batch loss for Iteration: 7 - Batch: 8 is 1.3459404815073788\n",
      "The batch loss for Iteration: 7 - Batch: 9 is 1.167859448083599\n",
      "The batch loss for Iteration: 7 - Batch: 10 is 1.0278777546131574\n",
      "The batch loss for Iteration: 7 - Batch: 11 is 0.9085144952473042\n",
      "The batch loss for Iteration: 7 - Batch: 12 is 0.799458026546217\n",
      "The batch loss for Iteration: 7 - Batch: 13 is 0.7103735037969675\n",
      "The batch loss for Iteration: 7 - Batch: 14 is 0.6323323953703187\n",
      "The batch loss for Iteration: 7 - Batch: 15 is 0.5579469545965092\n",
      "The batch loss for Iteration: 7 - Batch: 16 is 0.5305849964962561\n",
      "The batch loss for Iteration: 7 - Batch: 17 is 0.5039540764112503\n",
      "The batch loss for Iteration: 7 - Batch: 18 is 0.48361377463225336\n",
      "The batch loss for Iteration: 7 - Batch: 19 is 0.4710620783495326\n",
      "The batch loss for Iteration: 7 - Batch: 20 is 0.4588527856494104\n",
      "The batch loss for Iteration: 7 - Batch: 21 is 0.4495804101148569\n",
      "The batch loss for Iteration: 7 - Batch: 22 is 0.4438096431282954\n",
      "The batch loss for Iteration: 7 - Batch: 23 is 0.4254183030268707\n",
      "The batch loss for Iteration: 7 - Batch: 24 is 0.4065730065351373\n",
      "The batch loss for Iteration: 7 - Batch: 25 is 0.3890990732963484\n",
      "The batch loss for Iteration: 7 - Batch: 26 is 0.3761811415312305\n",
      "The batch loss for Iteration: 7 - Batch: 27 is 0.3633918256985205\n",
      "The batch loss for Iteration: 7 - Batch: 28 is 0.3539462894250164\n",
      "The batch loss for Iteration: 7 - Batch: 29 is 0.3455129491291086\n",
      "The batch loss for Iteration: 7 - Batch: 30 is 0.3365690350115993\n",
      "The batch loss for Iteration: 7 - Batch: 31 is 0.32893708981443415\n",
      "The batch loss for Iteration: 7 - Batch: 32 is 0.3220547629237684\n",
      "The batch loss for Iteration: 7 - Batch: 33 is 0.312791007970012\n",
      "The batch loss for Iteration: 7 - Batch: 34 is 0.3049225437701951\n",
      "The batch loss for Iteration: 7 - Batch: 35 is 0.2993457704453038\n",
      "The batch loss for Iteration: 7 - Batch: 36 is 0.29064175349546006\n",
      "The batch loss for Iteration: 7 - Batch: 37 is 0.28417825616819625\n",
      "The batch loss for Iteration: 7 - Batch: 38 is 0.2784275519331864\n",
      "The batch loss for Iteration: 7 - Batch: 39 is 0.2725272285627339\n",
      "The batch loss for Iteration: 7 - Batch: 40 is 0.2665753599775686\n",
      "The batch loss for Iteration: 7 - Batch: 41 is 0.2617247564375705\n",
      "The batch loss for Iteration: 7 - Batch: 42 is 0.25353552126787504\n",
      "The batch loss for Iteration: 7 - Batch: 43 is 0.2474962638237039\n",
      "The batch loss for Iteration: 7 - Batch: 44 is 0.23907883204004932\n",
      "The batch loss for Iteration: 7 - Batch: 45 is 0.23179585675333325\n",
      "The batch loss for Iteration: 7 - Batch: 46 is 0.22222465296584967\n",
      "The batch loss for Iteration: 7 - Batch: 47 is 0.2126835472923183\n",
      "The batch loss for Iteration: 7 - Batch: 48 is 0.20424050434824087\n",
      "The batch loss for Iteration: 7 - Batch: 49 is 0.19932078725981636\n",
      "The batch loss for Iteration: 7 - Batch: 50 is 0.193415978260073\n",
      "The batch loss for Iteration: 7 - Batch: 51 is 0.1897774357579311\n",
      "The batch loss for Iteration: 7 - Batch: 52 is 0.1860194667686122\n",
      "The batch loss for Iteration: 7 - Batch: 53 is 0.18347742907888562\n",
      "The batch loss for Iteration: 7 - Batch: 54 is 0.18235518765317435\n",
      "The batch loss for Iteration: 7 - Batch: 55 is 0.18058036520223303\n",
      "The batch loss for Iteration: 7 - Batch: 56 is 0.17606166767693063\n",
      "The batch loss for Iteration: 7 - Batch: 57 is 0.17429846795256393\n",
      "The batch loss for Iteration: 7 - Batch: 58 is 0.17177826170730312\n",
      "The batch loss for Iteration: 7 - Batch: 59 is 0.1708134740598678\n",
      "The batch loss for Iteration: 7 - Batch: 60 is 0.16982301976838346\n",
      "The batch loss for Iteration: 7 - Batch: 61 is 0.16805072511821642\n",
      "The batch loss for Iteration: 7 - Batch: 62 is 0.16502459022191304\n",
      "The batch loss for Iteration: 7 - Batch: 63 is 0.16252995766276915\n",
      "The batch loss for Iteration: 7 - Batch: 64 is 0.1581067013797202\n",
      "The batch loss for Iteration: 7 - Batch: 65 is 0.15511857946722352\n",
      "The batch loss for Iteration: 7 - Batch: 66 is 0.15450696261368116\n",
      "The batch loss for Iteration: 7 - Batch: 67 is 0.1545742300374312\n",
      "The batch loss for Iteration: 7 - Batch: 68 is 0.1544938409202096\n",
      "The batch loss for Iteration: 7 - Batch: 69 is 0.15524171466468745\n",
      "The batch loss for Iteration: 7 - Batch: 70 is 0.15593975679902947\n",
      "The batch loss for Iteration: 7 - Batch: 71 is 0.1580000147081123\n",
      "The batch loss for Iteration: 7 - Batch: 72 is 0.15737047617954847\n",
      "The batch loss for Iteration: 7 - Batch: 73 is 0.15280428325073714\n",
      "The batch loss for Iteration: 7 - Batch: 74 is 0.1490850563572429\n",
      "The batch loss for Iteration: 7 - Batch: 75 is 0.1453686012977657\n",
      "The batch loss for Iteration: 7 - Batch: 76 is 0.14105058678062743\n",
      "The batch loss for Iteration: 7 - Batch: 77 is 0.1367110259726608\n",
      "The batch loss for Iteration: 7 - Batch: 78 is 0.13229769574865882\n",
      "The batch loss for Iteration: 7 - Batch: 79 is 0.1303148530556839\n",
      "The batch loss for Iteration: 7 - Batch: 80 is 0.12723304639460384\n",
      "The batch loss for Iteration: 7 - Batch: 81 is 0.12460210287113037\n",
      "The batch loss for Iteration: 7 - Batch: 82 is 0.12303728970700227\n",
      "The batch loss for Iteration: 7 - Batch: 83 is 0.12174231607677606\n",
      "The batch loss for Iteration: 7 - Batch: 84 is 0.12149460815903744\n",
      "The batch loss for Iteration: 7 - Batch: 85 is 0.12107634356127198\n",
      "The batch loss for Iteration: 7 - Batch: 86 is 0.11924548256562885\n",
      "The batch loss for Iteration: 7 - Batch: 87 is 0.12010667487845353\n",
      "The batch loss for Iteration: 7 - Batch: 88 is 0.12004311140164739\n",
      "The batch loss for Iteration: 7 - Batch: 89 is 0.11927666479302612\n",
      "The batch loss for Iteration: 7 - Batch: 90 is 0.11816612335940783\n",
      "The batch loss for Iteration: 7 - Batch: 91 is 0.117591299813392\n",
      "The batch loss for Iteration: 7 - Batch: 92 is 0.1169031349796698\n",
      "The batch loss for Iteration: 7 - Batch: 93 is 0.1163283086499198\n",
      "The batch loss for Iteration: 7 - Batch: 94 is 0.11458769419389923\n",
      "The batch loss for Iteration: 7 - Batch: 95 is 0.11309479580349785\n",
      "The batch loss for Iteration: 7 - Batch: 96 is 0.1112692576202358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:02<05:29,  7.84s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 7 - Batch: 97 is 0.11079635691232484\n",
      "The batch loss for Iteration: 7 - Batch: 98 is 0.10994720845025667\n",
      "The batch loss for Iteration: 7 - Batch: 99 is 0.10807275775955139\n",
      "Iteration: 8\n",
      "The batch loss for Iteration: 8 - Batch: 0 is 9.306741714477539\n",
      "The batch loss for Iteration: 8 - Batch: 1 is 9.432146072387695\n",
      "The batch loss for Iteration: 8 - Batch: 2 is 6.388746897379558\n",
      "The batch loss for Iteration: 8 - Batch: 3 is 4.080797831217448\n",
      "The batch loss for Iteration: 8 - Batch: 4 is 2.8353902180989587\n",
      "The batch loss for Iteration: 8 - Batch: 5 is 2.191318056318495\n",
      "The batch loss for Iteration: 8 - Batch: 6 is 1.8073432816399468\n",
      "The batch loss for Iteration: 8 - Batch: 7 is 1.5469323145018683\n",
      "The batch loss for Iteration: 8 - Batch: 8 is 1.3459407063178075\n",
      "The batch loss for Iteration: 8 - Batch: 9 is 1.1678591844623472\n",
      "The batch loss for Iteration: 8 - Batch: 10 is 1.0278772104615983\n",
      "The batch loss for Iteration: 8 - Batch: 11 is 0.908514132009902\n",
      "The batch loss for Iteration: 8 - Batch: 12 is 0.7994575584475014\n",
      "The batch loss for Iteration: 8 - Batch: 13 is 0.710373470361345\n",
      "The batch loss for Iteration: 8 - Batch: 14 is 0.6323318845149751\n",
      "The batch loss for Iteration: 8 - Batch: 15 is 0.5579470418773398\n",
      "The batch loss for Iteration: 8 - Batch: 16 is 0.5305850016304225\n",
      "The batch loss for Iteration: 8 - Batch: 17 is 0.5039539707326689\n",
      "The batch loss for Iteration: 8 - Batch: 18 is 0.4836137690702227\n",
      "The batch loss for Iteration: 8 - Batch: 19 is 0.4710619827039994\n",
      "The batch loss for Iteration: 8 - Batch: 20 is 0.45885287192098656\n",
      "The batch loss for Iteration: 8 - Batch: 21 is 0.44958037068745965\n",
      "The batch loss for Iteration: 8 - Batch: 22 is 0.44380964141406076\n",
      "The batch loss for Iteration: 8 - Batch: 23 is 0.4254183029554443\n",
      "The batch loss for Iteration: 8 - Batch: 24 is 0.40657308282622556\n",
      "The batch loss for Iteration: 8 - Batch: 25 is 0.38909900287105825\n",
      "The batch loss for Iteration: 8 - Batch: 26 is 0.3761811389228864\n",
      "The batch loss for Iteration: 8 - Batch: 27 is 0.36339175748577135\n",
      "The batch loss for Iteration: 8 - Batch: 28 is 0.35394635284349507\n",
      "The batch loss for Iteration: 8 - Batch: 29 is 0.3455129830322018\n",
      "The batch loss for Iteration: 8 - Batch: 30 is 0.3365691283963103\n",
      "The batch loss for Iteration: 8 - Batch: 31 is 0.3289370927327063\n",
      "The batch loss for Iteration: 8 - Batch: 32 is 0.3220547630122009\n",
      "The batch loss for Iteration: 8 - Batch: 33 is 0.312791007972613\n",
      "The batch loss for Iteration: 8 - Batch: 34 is 0.3049225165224318\n",
      "The batch loss for Iteration: 8 - Batch: 35 is 0.29934576968842147\n",
      "The batch loss for Iteration: 8 - Batch: 36 is 0.2906418565749299\n",
      "The batch loss for Iteration: 8 - Batch: 37 is 0.28417833417089144\n",
      "The batch loss for Iteration: 8 - Batch: 38 is 0.27842762729281834\n",
      "The batch loss for Iteration: 8 - Batch: 39 is 0.2725272304467247\n",
      "The batch loss for Iteration: 8 - Batch: 40 is 0.2665752204614245\n",
      "The batch loss for Iteration: 8 - Batch: 41 is 0.26172470770269485\n",
      "The batch loss for Iteration: 8 - Batch: 42 is 0.2535356088483957\n",
      "The batch loss for Iteration: 8 - Batch: 43 is 0.24749628748858657\n",
      "The batch loss for Iteration: 8 - Batch: 44 is 0.23907876898764782\n",
      "The batch loss for Iteration: 8 - Batch: 45 is 0.23179577245442748\n",
      "The batch loss for Iteration: 8 - Batch: 46 is 0.22222442797188402\n",
      "The batch loss for Iteration: 8 - Batch: 47 is 0.2126832644499351\n",
      "The batch loss for Iteration: 8 - Batch: 48 is 0.20424026502305354\n",
      "The batch loss for Iteration: 8 - Batch: 49 is 0.1993207061793673\n",
      "The batch loss for Iteration: 8 - Batch: 50 is 0.19341595797076386\n",
      "The batch loss for Iteration: 8 - Batch: 51 is 0.18977743536775207\n",
      "The batch loss for Iteration: 8 - Batch: 52 is 0.1860195027489604\n",
      "The batch loss for Iteration: 8 - Batch: 53 is 0.18347746506645934\n",
      "The batch loss for Iteration: 8 - Batch: 54 is 0.18235518830749387\n",
      "The batch loss for Iteration: 8 - Batch: 55 is 0.18058034818401877\n",
      "The batch loss for Iteration: 8 - Batch: 56 is 0.17606166737836548\n",
      "The batch loss for Iteration: 8 - Batch: 57 is 0.17429848439007686\n",
      "The batch loss for Iteration: 8 - Batch: 58 is 0.17177826198590504\n",
      "The batch loss for Iteration: 8 - Batch: 59 is 0.17081347406451117\n",
      "The batch loss for Iteration: 8 - Batch: 60 is 0.16982305103646997\n",
      "The batch loss for Iteration: 8 - Batch: 61 is 0.16805074100438422\n",
      "The batch loss for Iteration: 8 - Batch: 62 is 0.1650245601986993\n",
      "The batch loss for Iteration: 8 - Batch: 63 is 0.16252994229249523\n",
      "The batch loss for Iteration: 8 - Batch: 64 is 0.15810670114325445\n",
      "The batch loss for Iteration: 8 - Batch: 65 is 0.1551185794636407\n",
      "The batch loss for Iteration: 8 - Batch: 66 is 0.15450699108151775\n",
      "The batch loss for Iteration: 8 - Batch: 67 is 0.15457424448069815\n",
      "The batch loss for Iteration: 8 - Batch: 68 is 0.15449386877226612\n",
      "The batch loss for Iteration: 8 - Batch: 69 is 0.15524172868649277\n",
      "The batch loss for Iteration: 8 - Batch: 70 is 0.15593978386058496\n",
      "The batch loss for Iteration: 8 - Batch: 71 is 0.15800000183849067\n",
      "The batch loss for Iteration: 8 - Batch: 72 is 0.15737048906728401\n",
      "The batch loss for Iteration: 8 - Batch: 73 is 0.15280432208736802\n",
      "The batch loss for Iteration: 8 - Batch: 74 is 0.14908503144374954\n",
      "The batch loss for Iteration: 8 - Batch: 75 is 0.14536861351830285\n",
      "The batch loss for Iteration: 8 - Batch: 76 is 0.14105056216857423\n",
      "The batch loss for Iteration: 8 - Batch: 77 is 0.13671102565712165\n",
      "The batch loss for Iteration: 8 - Batch: 78 is 0.13229770781649144\n",
      "The batch loss for Iteration: 8 - Batch: 79 is 0.13031482936467392\n",
      "The batch loss for Iteration: 8 - Batch: 80 is 0.1272330696496362\n",
      "The batch loss for Iteration: 8 - Batch: 81 is 0.12460210315472832\n",
      "The batch loss for Iteration: 8 - Batch: 82 is 0.12303728971041912\n",
      "The batch loss for Iteration: 8 - Batch: 83 is 0.12174232743008243\n",
      "The batch loss for Iteration: 8 - Batch: 84 is 0.12149460829260575\n",
      "The batch loss for Iteration: 8 - Batch: 85 is 0.12107634356282508\n",
      "The batch loss for Iteration: 8 - Batch: 86 is 0.11924546064209919\n",
      "The batch loss for Iteration: 8 - Batch: 87 is 0.12010667462932251\n",
      "The batch loss for Iteration: 8 - Batch: 88 is 0.12004310068340641\n",
      "The batch loss for Iteration: 8 - Batch: 89 is 0.11927667527031585\n",
      "The batch loss for Iteration: 8 - Batch: 90 is 0.11816612347454288\n",
      "The batch loss for Iteration: 8 - Batch: 91 is 0.11759129981464347\n",
      "The batch loss for Iteration: 8 - Batch: 92 is 0.11690313497968324\n",
      "The batch loss for Iteration: 8 - Batch: 93 is 0.1163283187953914\n",
      "The batch loss for Iteration: 8 - Batch: 94 is 0.1145877143780477\n",
      "The batch loss for Iteration: 8 - Batch: 95 is 0.1130947960137494\n",
      "The batch loss for Iteration: 8 - Batch: 96 is 0.1112692871174853\n",
      "The batch loss for Iteration: 8 - Batch: 97 is 0.11079636694468775\n",
      "The batch loss for Iteration: 8 - Batch: 98 is 0.10994720855159368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:10<05:21,  7.85s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 8 - Batch: 99 is 0.10807275776056477\n",
      "Iteration: 9\n",
      "The batch loss for Iteration: 9 - Batch: 0 is 9.306740760803223\n",
      "The batch loss for Iteration: 9 - Batch: 1 is 9.432145595550537\n",
      "The batch loss for Iteration: 9 - Batch: 2 is 6.388747374216716\n",
      "The batch loss for Iteration: 9 - Batch: 3 is 4.080797950426738\n",
      "The batch loss for Iteration: 9 - Batch: 4 is 2.83538986047109\n",
      "The batch loss for Iteration: 9 - Batch: 5 is 2.1913181556595696\n",
      "The batch loss for Iteration: 9 - Batch: 6 is 1.8073431595923408\n",
      "The batch loss for Iteration: 9 - Batch: 7 is 1.5469325376644967\n",
      "The batch loss for Iteration: 9 - Batch: 8 is 1.3459403072584033\n",
      "The batch loss for Iteration: 9 - Batch: 9 is 1.167859049188975\n",
      "The batch loss for Iteration: 9 - Batch: 10 is 1.0278772848616842\n",
      "The batch loss for Iteration: 9 - Batch: 11 is 0.9085138997913301\n",
      "The batch loss for Iteration: 9 - Batch: 12 is 0.7994575405845344\n",
      "The batch loss for Iteration: 9 - Batch: 13 is 0.7103736053246068\n",
      "The batch loss for Iteration: 9 - Batch: 14 is 0.6323317663559503\n",
      "The batch loss for Iteration: 9 - Batch: 15 is 0.5579469152831111\n",
      "The batch loss for Iteration: 9 - Batch: 16 is 0.5305849941837032\n",
      "The batch loss for Iteration: 9 - Batch: 17 is 0.5039540762827752\n",
      "The batch loss for Iteration: 9 - Batch: 18 is 0.4836137746254915\n",
      "The batch loss for Iteration: 9 - Batch: 19 is 0.471061935298047\n",
      "The batch loss for Iteration: 9 - Batch: 20 is 0.4588527788374349\n",
      "The batch loss for Iteration: 9 - Batch: 21 is 0.44958045315405426\n",
      "The batch loss for Iteration: 9 - Batch: 22 is 0.4438095620713634\n",
      "The batch loss for Iteration: 9 - Batch: 23 is 0.42541829964949857\n",
      "The batch loss for Iteration: 9 - Batch: 24 is 0.40657300640004246\n",
      "The batch loss for Iteration: 9 - Batch: 25 is 0.3890991099709339\n",
      "The batch loss for Iteration: 9 - Batch: 26 is 0.37618121353209044\n",
      "The batch loss for Iteration: 9 - Batch: 27 is 0.36339169203079175\n",
      "The batch loss for Iteration: 9 - Batch: 28 is 0.3539463177011056\n",
      "The batch loss for Iteration: 9 - Batch: 29 is 0.34551301364993264\n",
      "The batch loss for Iteration: 9 - Batch: 30 is 0.33656909862029144\n",
      "The batch loss for Iteration: 9 - Batch: 31 is 0.3289370023952386\n",
      "The batch loss for Iteration: 9 - Batch: 32 is 0.32205476027470187\n",
      "The batch loss for Iteration: 9 - Batch: 33 is 0.3127910078920983\n",
      "The batch loss for Iteration: 9 - Batch: 34 is 0.30492248927229376\n",
      "The batch loss for Iteration: 9 - Batch: 35 is 0.29934582191337966\n",
      "The batch loss for Iteration: 9 - Batch: 36 is 0.29064180643645215\n",
      "The batch loss for Iteration: 9 - Batch: 37 is 0.2841783077547653\n",
      "The batch loss for Iteration: 9 - Batch: 38 is 0.27842762661548176\n",
      "The batch loss for Iteration: 9 - Batch: 39 is 0.2725272304297913\n",
      "The batch loss for Iteration: 9 - Batch: 40 is 0.26657524372136066\n",
      "The batch loss for Iteration: 9 - Batch: 41 is 0.2617247990826282\n",
      "The batch loss for Iteration: 9 - Batch: 42 is 0.2535355666165655\n",
      "The batch loss for Iteration: 9 - Batch: 43 is 0.24749632987760484\n",
      "The batch loss for Iteration: 9 - Batch: 44 is 0.23907883350791378\n",
      "The batch loss for Iteration: 9 - Batch: 45 is 0.23179581532114266\n",
      "The batch loss for Iteration: 9 - Batch: 46 is 0.22222455062959917\n",
      "The batch loss for Iteration: 9 - Batch: 47 is 0.21268336634637877\n",
      "The batch loss for Iteration: 9 - Batch: 48 is 0.2042403449535374\n",
      "The batch loss for Iteration: 9 - Batch: 49 is 0.199320707777977\n",
      "The batch loss for Iteration: 9 - Batch: 50 is 0.19341597670160554\n",
      "The batch loss for Iteration: 9 - Batch: 51 is 0.18977739904817914\n",
      "The batch loss for Iteration: 9 - Batch: 52 is 0.18601950206368542\n",
      "The batch loss for Iteration: 9 - Batch: 53 is 0.18347746505376905\n",
      "The batch loss for Iteration: 9 - Batch: 54 is 0.18235518830726313\n",
      "The batch loss for Iteration: 9 - Batch: 55 is 0.18058036521391316\n",
      "The batch loss for Iteration: 9 - Batch: 56 is 0.17606166767713555\n",
      "The batch loss for Iteration: 9 - Batch: 57 is 0.17429850083788873\n",
      "The batch loss for Iteration: 9 - Batch: 58 is 0.17177822993673858\n",
      "The batch loss for Iteration: 9 - Batch: 59 is 0.1708135212140742\n",
      "The batch loss for Iteration: 9 - Batch: 60 is 0.1698230518094136\n",
      "The batch loss for Iteration: 9 - Batch: 61 is 0.1680507102531634\n",
      "The batch loss for Iteration: 9 - Batch: 62 is 0.16502462026133494\n",
      "The batch loss for Iteration: 9 - Batch: 63 is 0.16252992832981272\n",
      "The batch loss for Iteration: 9 - Batch: 64 is 0.15810674494418162\n",
      "The batch loss for Iteration: 9 - Batch: 65 is 0.1551185801272911\n",
      "The batch loss for Iteration: 9 - Batch: 66 is 0.15450704802720305\n",
      "The batch loss for Iteration: 9 - Batch: 67 is 0.154574259342757\n",
      "The batch loss for Iteration: 9 - Batch: 68 is 0.15449385516629138\n",
      "The batch loss for Iteration: 9 - Batch: 69 is 0.15524170124428407\n",
      "The batch loss for Iteration: 9 - Batch: 70 is 0.1559397431779771\n",
      "The batch loss for Iteration: 9 - Batch: 71 is 0.1580000410098843\n",
      "The batch loss for Iteration: 9 - Batch: 72 is 0.15737045041178327\n",
      "The batch loss for Iteration: 9 - Batch: 73 is 0.1528042829025241\n",
      "The batch loss for Iteration: 9 - Batch: 74 is 0.1490850436369425\n",
      "The batch loss for Iteration: 9 - Batch: 75 is 0.14536863877543213\n",
      "The batch loss for Iteration: 9 - Batch: 76 is 0.14105058726735037\n",
      "The batch loss for Iteration: 9 - Batch: 77 is 0.13671101375230704\n",
      "The batch loss for Iteration: 9 - Batch: 78 is 0.13229768352214402\n",
      "The batch loss for Iteration: 9 - Batch: 79 is 0.13031484098192353\n",
      "The batch loss for Iteration: 9 - Batch: 80 is 0.12723303447178808\n",
      "The batch loss for Iteration: 9 - Batch: 81 is 0.12460210272573016\n",
      "The batch loss for Iteration: 9 - Batch: 82 is 0.12303730119530247\n",
      "The batch loss for Iteration: 9 - Batch: 83 is 0.12174231621354153\n",
      "The batch loss for Iteration: 9 - Batch: 84 is 0.12149460816064644\n",
      "The batch loss for Iteration: 9 - Batch: 85 is 0.12107637682899938\n",
      "The batch loss for Iteration: 9 - Batch: 86 is 0.11924546102446901\n",
      "The batch loss for Iteration: 9 - Batch: 87 is 0.12010666379645948\n",
      "The batch loss for Iteration: 9 - Batch: 88 is 0.1200431112771306\n",
      "The batch loss for Iteration: 9 - Batch: 89 is 0.11927667538802389\n",
      "The batch loss for Iteration: 9 - Batch: 90 is 0.11816611299589883\n",
      "The batch loss for Iteration: 9 - Batch: 91 is 0.11759132043279552\n",
      "The batch loss for Iteration: 9 - Batch: 92 is 0.1169031352013838\n",
      "The batch loss for Iteration: 9 - Batch: 93 is 0.11632833908869282\n",
      "The batch loss for Iteration: 9 - Batch: 94 is 0.11458769451430736\n",
      "The batch loss for Iteration: 9 - Batch: 95 is 0.11309478587272798\n",
      "The batch loss for Iteration: 9 - Batch: 96 is 0.11126925751785673\n",
      "The batch loss for Iteration: 9 - Batch: 97 is 0.11079634717990956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:18<05:16,  7.92s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 9 - Batch: 98 is 0.10994722761809726\n",
      "The batch loss for Iteration: 9 - Batch: 99 is 0.10807276748797295\n",
      "Iteration: 10\n",
      "The batch loss for Iteration: 10 - Batch: 0 is 9.306741714477539\n",
      "The batch loss for Iteration: 10 - Batch: 1 is 9.432147026062012\n",
      "The batch loss for Iteration: 10 - Batch: 2 is 6.388747851053874\n",
      "The batch loss for Iteration: 10 - Batch: 3 is 4.080798069636027\n",
      "The batch loss for Iteration: 10 - Batch: 4 is 2.8353895028432214\n",
      "The batch loss for Iteration: 10 - Batch: 5 is 2.191318096054925\n",
      "The batch loss for Iteration: 10 - Batch: 6 is 1.8073432873165796\n",
      "The batch loss for Iteration: 10 - Batch: 7 is 1.5469323152114474\n",
      "The batch loss for Iteration: 10 - Batch: 8 is 1.3459407063966495\n",
      "The batch loss for Iteration: 10 - Batch: 9 is 1.1678588030005048\n",
      "The batch loss for Iteration: 10 - Batch: 10 is 1.0278770890855837\n",
      "The batch loss for Iteration: 10 - Batch: 11 is 0.9085138834766551\n",
      "The batch loss for Iteration: 10 - Batch: 12 is 0.7994576126891222\n",
      "The batch loss for Iteration: 10 - Batch: 13 is 0.7103738829533107\n",
      "The batch loss for Iteration: 10 - Batch: 14 is 0.6323315941296673\n",
      "The batch loss for Iteration: 10 - Batch: 15 is 0.5579471429375475\n",
      "The batch loss for Iteration: 10 - Batch: 16 is 0.5305850075751406\n",
      "The batch loss for Iteration: 10 - Batch: 17 is 0.5039540240448375\n",
      "The batch loss for Iteration: 10 - Batch: 18 is 0.4836137718761263\n",
      "The batch loss for Iteration: 10 - Batch: 19 is 0.4710617921094313\n",
      "The batch loss for Iteration: 10 - Batch: 20 is 0.4588528628450548\n",
      "The batch loss for Iteration: 10 - Batch: 21 is 0.44958037027491726\n",
      "The batch loss for Iteration: 10 - Batch: 22 is 0.4438096413961241\n",
      "The batch loss for Iteration: 10 - Batch: 23 is 0.4254183029546969\n",
      "The batch loss for Iteration: 10 - Batch: 24 is 0.40657300653225037\n",
      "The batch loss for Iteration: 10 - Batch: 25 is 0.3890991466558002\n",
      "The batch loss for Iteration: 10 - Batch: 26 is 0.37618110892697626\n",
      "The batch loss for Iteration: 10 - Batch: 27 is 0.3633915520557068\n",
      "The batch loss for Iteration: 10 - Batch: 28 is 0.3539462799890572\n",
      "The batch loss for Iteration: 10 - Batch: 29 is 0.34551291702543274\n",
      "The batch loss for Iteration: 10 - Batch: 30 is 0.33656897244862155\n",
      "The batch loss for Iteration: 10 - Batch: 31 is 0.3289369388477291\n",
      "The batch loss for Iteration: 10 - Batch: 32 is 0.32205493174434996\n",
      "The batch loss for Iteration: 10 - Batch: 33 is 0.31279095683683406\n",
      "The batch loss for Iteration: 10 - Batch: 34 is 0.3049224060700591\n",
      "The batch loss for Iteration: 10 - Batch: 35 is 0.2993458196022065\n",
      "The batch loss for Iteration: 10 - Batch: 36 is 0.29064175482402504\n",
      "The batch loss for Iteration: 10 - Batch: 37 is 0.2841782311064659\n",
      "The batch loss for Iteration: 10 - Batch: 38 is 0.27842755129057795\n",
      "The batch loss for Iteration: 10 - Batch: 39 is 0.2725272762303846\n",
      "The batch loss for Iteration: 10 - Batch: 40 is 0.26657517505740075\n",
      "The batch loss for Iteration: 10 - Batch: 41 is 0.2617249336869601\n",
      "The batch loss for Iteration: 10 - Batch: 42 is 0.2535356584607887\n",
      "The batch loss for Iteration: 10 - Batch: 43 is 0.24749628861614098\n",
      "The batch loss for Iteration: 10 - Batch: 44 is 0.23907904451861822\n",
      "The batch loss for Iteration: 10 - Batch: 45 is 0.23179602722883547\n",
      "The batch loss for Iteration: 10 - Batch: 46 is 0.22222490008430285\n",
      "The batch loss for Iteration: 10 - Batch: 47 is 0.21268379085919853\n",
      "The batch loss for Iteration: 10 - Batch: 48 is 0.20424050931899354\n",
      "The batch loss for Iteration: 10 - Batch: 49 is 0.19932073013877244\n",
      "The batch loss for Iteration: 10 - Batch: 50 is 0.19341597714005254\n",
      "The batch loss for Iteration: 10 - Batch: 51 is 0.18977732569704803\n",
      "The batch loss for Iteration: 10 - Batch: 52 is 0.18601953666741186\n",
      "The batch loss for Iteration: 10 - Batch: 53 is 0.1834775186764853\n",
      "The batch loss for Iteration: 10 - Batch: 54 is 0.18235529331941974\n",
      "The batch loss for Iteration: 10 - Batch: 55 is 0.18058035005923176\n",
      "The batch loss for Iteration: 10 - Batch: 56 is 0.17606163394900723\n",
      "The batch loss for Iteration: 10 - Batch: 57 is 0.17429854958435115\n",
      "The batch loss for Iteration: 10 - Batch: 58 is 0.17177818227103542\n",
      "The batch loss for Iteration: 10 - Batch: 59 is 0.17081363168164942\n",
      "The batch loss for Iteration: 10 - Batch: 60 is 0.16982308488836786\n",
      "The batch loss for Iteration: 10 - Batch: 61 is 0.16805071078669495\n",
      "The batch loss for Iteration: 10 - Batch: 62 is 0.16502463540749124\n",
      "The batch loss for Iteration: 10 - Batch: 63 is 0.16252988386298783\n",
      "The batch loss for Iteration: 10 - Batch: 64 is 0.15810680294772686\n",
      "The batch loss for Iteration: 10 - Batch: 65 is 0.15511855210691097\n",
      "The batch loss for Iteration: 10 - Batch: 66 is 0.15450713301265853\n",
      "The batch loss for Iteration: 10 - Batch: 67 is 0.15457426059254312\n",
      "The batch loss for Iteration: 10 - Batch: 68 is 0.1544937584348359\n",
      "The batch loss for Iteration: 10 - Batch: 69 is 0.15524156362321806\n",
      "The batch loss for Iteration: 10 - Batch: 70 is 0.15593966064745646\n",
      "The batch loss for Iteration: 10 - Batch: 71 is 0.15800002661815044\n",
      "The batch loss for Iteration: 10 - Batch: 72 is 0.1573703979585093\n",
      "The batch loss for Iteration: 10 - Batch: 73 is 0.15280432085616835\n",
      "The batch loss for Iteration: 10 - Batch: 74 is 0.1490851712995666\n",
      "The batch loss for Iteration: 10 - Batch: 75 is 0.14536880358370496\n",
      "The batch loss for Iteration: 10 - Batch: 76 is 0.14105076280304782\n",
      "The batch loss for Iteration: 10 - Batch: 77 is 0.13671106490913987\n",
      "The batch loss for Iteration: 10 - Batch: 78 is 0.13229770831335244\n",
      "The batch loss for Iteration: 10 - Batch: 79 is 0.13031486513367155\n",
      "The batch loss for Iteration: 10 - Batch: 80 is 0.12723307009122878\n",
      "The batch loss for Iteration: 10 - Batch: 81 is 0.1246021031601136\n",
      "The batch loss for Iteration: 10 - Batch: 82 is 0.12303730120053599\n",
      "The batch loss for Iteration: 10 - Batch: 83 is 0.12174229350707251\n",
      "The batch loss for Iteration: 10 - Batch: 84 is 0.12149461911320936\n",
      "The batch loss for Iteration: 10 - Batch: 85 is 0.12107643240253595\n",
      "The batch loss for Iteration: 10 - Batch: 86 is 0.1192455055103403\n",
      "The batch loss for Iteration: 10 - Batch: 87 is 0.1201066534647726\n",
      "The batch loss for Iteration: 10 - Batch: 88 is 0.12004311116104423\n",
      "The batch loss for Iteration: 10 - Batch: 89 is 0.11927664359759016\n",
      "The batch loss for Iteration: 10 - Batch: 90 is 0.11816614408636604\n",
      "The batch loss for Iteration: 10 - Batch: 91 is 0.11759138296688645\n",
      "The batch loss for Iteration: 10 - Batch: 92 is 0.11690307434641813\n",
      "The batch loss for Iteration: 10 - Batch: 93 is 0.11632833844129957\n",
      "The batch loss for Iteration: 10 - Batch: 94 is 0.11458770454616972\n",
      "The batch loss for Iteration: 10 - Batch: 95 is 0.11309482571365638\n",
      "The batch loss for Iteration: 10 - Batch: 96 is 0.11126921860181202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:26<05:08,  7.90s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 10 - Batch: 97 is 0.1107963370514365\n",
      "The batch loss for Iteration: 10 - Batch: 98 is 0.10994724678193725\n",
      "The batch loss for Iteration: 10 - Batch: 99 is 0.10807279628984086\n",
      "Iteration: 11\n",
      "The batch loss for Iteration: 11 - Batch: 0 is 9.306743621826172\n",
      "The batch loss for Iteration: 11 - Batch: 1 is 9.432147026062012\n",
      "The batch loss for Iteration: 11 - Batch: 2 is 6.388747851053874\n",
      "The batch loss for Iteration: 11 - Batch: 3 is 4.0807985464731855\n",
      "The batch loss for Iteration: 11 - Batch: 4 is 2.8353901704152427\n",
      "The batch loss for Iteration: 11 - Batch: 5 is 2.19131788942549\n",
      "The batch loss for Iteration: 11 - Batch: 6 is 1.8073424403629605\n",
      "The batch loss for Iteration: 11 - Batch: 7 is 1.5469329245979824\n",
      "The batch loss for Iteration: 11 - Batch: 8 is 1.3459403502510128\n",
      "The batch loss for Iteration: 11 - Batch: 9 is 1.167859053488236\n",
      "The batch loss for Iteration: 11 - Batch: 10 is 1.027877198554861\n",
      "The batch loss for Iteration: 11 - Batch: 11 is 0.9085138131262352\n",
      "The batch loss for Iteration: 11 - Batch: 12 is 0.7994575339179887\n",
      "The batch loss for Iteration: 11 - Batch: 13 is 0.710373604848425\n",
      "The batch loss for Iteration: 11 - Batch: 14 is 0.6323313212761905\n",
      "The batch loss for Iteration: 11 - Batch: 15 is 0.5579470066749157\n",
      "The batch loss for Iteration: 11 - Batch: 16 is 0.5305852239536485\n",
      "The batch loss for Iteration: 11 - Batch: 17 is 0.5039543539573045\n",
      "The batch loss for Iteration: 11 - Batch: 18 is 0.48361388962671054\n",
      "The batch loss for Iteration: 11 - Batch: 19 is 0.4710616072620972\n",
      "The batch loss for Iteration: 11 - Batch: 20 is 0.45885294486892614\n",
      "The batch loss for Iteration: 11 - Batch: 21 is 0.4495811542822612\n",
      "The batch loss for Iteration: 11 - Batch: 22 is 0.44380967548339995\n",
      "The batch loss for Iteration: 11 - Batch: 23 is 0.4254183043750001\n",
      "The batch loss for Iteration: 11 - Batch: 24 is 0.40657300658906254\n",
      "The batch loss for Iteration: 11 - Batch: 25 is 0.3890990732984225\n",
      "The batch loss for Iteration: 11 - Batch: 26 is 0.3761812121738493\n",
      "The batch loss for Iteration: 11 - Batch: 27 is 0.36339179416167416\n",
      "The batch loss for Iteration: 11 - Batch: 28 is 0.3539461896815751\n",
      "The batch loss for Iteration: 11 - Batch: 29 is 0.3455128504368956\n",
      "The batch loss for Iteration: 11 - Batch: 30 is 0.33656897030060423\n",
      "The batch loss for Iteration: 11 - Batch: 31 is 0.32893699838524837\n",
      "The batch loss for Iteration: 11 - Batch: 32 is 0.3220548468508521\n",
      "The batch loss for Iteration: 11 - Batch: 33 is 0.3127910104384557\n",
      "The batch loss for Iteration: 11 - Batch: 34 is 0.30492251659288444\n",
      "The batch loss for Iteration: 11 - Batch: 35 is 0.2993457431994253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:29<05:17,  8.14s/it, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch loss for Iteration: 11 - Batch: 36 is 0.29064164965915884\n",
      "The batch loss for Iteration: 11 - Batch: 37 is 0.2841781279521993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericliu/Desktop/ML-Side Project/TS Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m l_r \u001b[39m=\u001b[39m \u001b[39m0.00079\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m p_model\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     input_tensor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     target_tensor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     n_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     target_len,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49ml_r,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/ML-Side-Project-HTS-Forecasting/notebook/feature_generation_training.ipynb#Y116sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ML-Side Project/TS Forecasting/ML-Side-Project-HTS-Forecasting/src/proption_model.py:457\u001b[0m, in \u001b[0;36mproportion_model.train\u001b[0;34m(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m# print(f\"The decoder input shape is {decoder_input.shape}\")\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(target_len):\n\u001b[1;32m    451\u001b[0m     \u001b[39m# -------- recursive prediction----------\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[39m# print(t)\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     (\n\u001b[1;32m    454\u001b[0m         layer_output,\n\u001b[1;32m    455\u001b[0m         decoder_output,\n\u001b[1;32m    456\u001b[0m         (decoder_hn, decoder_cn),\n\u001b[0;32m--> 457\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder_lstm\u001b[39m.\u001b[39;49mforward(decoder_input, decoder_cache)\n\u001b[1;32m    459\u001b[0m     \u001b[39m# print(decoder_output.shape)\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39m# print(layer_output.shape)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     decoder_ouputs[t, :, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(layer_output, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ML-Side Project/TS Forecasting/ML-Side-Project-HTS-Forecasting/src/proption_model.py:181\u001b[0m, in \u001b[0;36mdecoder_lstm.forward\u001b[0;34m(self, x, encoder_cache)\u001b[0m\n\u001b[1;32m    172\u001b[0m (hn, cn) \u001b[39m=\u001b[39m encoder_cache\n\u001b[1;32m    174\u001b[0m \u001b[39m### commented the line due to the fact that we are running a batched-decoder LSTM\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m# if self.batch_first:\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39m#     x = x.unsqueeze(1)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m#     x = x.unsqueeze(0)\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m decoder_output, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhn, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcn) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (hn, cn))\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhn, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcn)\n\u001b[1;32m    183\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_1(decoder_output)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    760\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    762\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    765\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m format_list(extract_stack(f, limit\u001b[39m=\u001b[39;49mlimit))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[1;32m    212\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    359\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals))\n\u001b[1;32m    361\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[0;32m--> 362\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[1;32m    363\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/compilerop.py:193\u001b[0m, in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\"\"\"Call linecache.checkcache() safely protecting our cached values.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m# First call the original checkcache as intended\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m linecache\u001b[39m.\u001b[39;49m_checkcache_ori(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    194\u001b[0m \u001b[39m# Then, update back the cache with our data, so that tracebacks related\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# to our compiled codes can be produced.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m linecache\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mupdate(linecache\u001b[39m.\u001b[39m_ipython_cache)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/linecache.py:85\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fullname)\n\u001b[1;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "### convert np array into torch array \n",
    "input_tensor = torch.tensor(input_array).float()\n",
    "target_tensor = torch.tensor(target_array).float()\n",
    "\n",
    "###---------- dimension on the model hypter-parameters from the paper ------------ ######\n",
    "no_child = 3060 \n",
    "num_hts_embedd = no_child\n",
    "hts_embedd_dim = 8\n",
    "covariate_dim = 0\n",
    "\n",
    "lstm_input_dim = 2 + covariate_dim + hts_embedd_dim\n",
    "lstm_hidden_dim = 48\n",
    "lstm_num_layer = 1\n",
    "lstm_output_dim = 64\n",
    "\n",
    "mha_embedd_dim = lstm_output_dim\n",
    "num_head = 4\n",
    "num_attention_layer = 1\n",
    "mha_output_dim = mha_embedd_dim\n",
    "residual_output_dim = mha_output_dim\n",
    "model_ouput_dim = 1\n",
    "\n",
    "# define the model object\n",
    "p_model = proption_model.proportion_model(\n",
    "    num_hts_embedd,\n",
    "    hts_embedd_dim,  # ts embedding hyper pars\n",
    "    lstm_input_dim,\n",
    "    lstm_hidden_dim,\n",
    "    lstm_num_layer,\n",
    "    lstm_output_dim,  # lstm hyper pars\n",
    "    mha_embedd_dim,\n",
    "    num_head,\n",
    "    num_attention_layer,  # mha hyper pars\n",
    "    mha_output_dim,\n",
    "    residual_output_dim,  # skip connection hyper pars\n",
    "    model_ouput_dim,  # output later hyper pars\n",
    ")\n",
    "\n",
    "###---------- trainign parameters from the paper ------------ ######\n",
    "\n",
    "n_epochs = 50\n",
    "target_len = Forward\n",
    "batch_size = 1\n",
    "l_r = 0.00079\n",
    "\n",
    "# start training\n",
    "p_model.train(\n",
    "    input_tensor,\n",
    "    target_tensor,\n",
    "    n_epochs,\n",
    "    target_len,\n",
    "    batch_size,\n",
    "    learning_rate=l_r,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## PROD CODE FOLLOWING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_002</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_003</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_004</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_005</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat_id  dept_id      item_id  id\n",
       "0  FOODS  FOODS_1  FOODS_1_001  10\n",
       "1  FOODS  FOODS_1  FOODS_1_002  10\n",
       "2  FOODS  FOODS_1  FOODS_1_003  10\n",
       "3  FOODS  FOODS_1  FOODS_1_004  10\n",
       "4  FOODS  FOODS_1  FOODS_1_005  10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_prod = sales_train_validation.groupby(groupby_list[2:]).count()[['id']].reset_index()\n",
    "hl_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = hl_prod\n",
    "df_right = sales_train_validation.groupby('item_id').sum()\n",
    "df = df_left.merge(df_right, left_on='item_id', right_index=True, how='inner').drop(columns=['id'])\n",
    "df.rename(columns=d_to_date, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>2011-01-29</th>\n",
       "      <th>2011-01-30</th>\n",
       "      <th>2011-01-31</th>\n",
       "      <th>2011-02-01</th>\n",
       "      <th>2011-02-02</th>\n",
       "      <th>2011-02-03</th>\n",
       "      <th>2011-02-04</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-04-15</th>\n",
       "      <th>2016-04-16</th>\n",
       "      <th>2016-04-17</th>\n",
       "      <th>2016-04-18</th>\n",
       "      <th>2016-04-19</th>\n",
       "      <th>2016-04-20</th>\n",
       "      <th>2016-04-21</th>\n",
       "      <th>2016-04-22</th>\n",
       "      <th>2016-04-23</th>\n",
       "      <th>2016-04-24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_002</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_003</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_005</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>HOUSEHOLD_2</td>\n",
       "      <td>HOUSEHOLD_2_512</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>HOUSEHOLD_2</td>\n",
       "      <td>HOUSEHOLD_2_513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>HOUSEHOLD_2</td>\n",
       "      <td>HOUSEHOLD_2_514</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>HOUSEHOLD_2</td>\n",
       "      <td>HOUSEHOLD_2_515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>HOUSEHOLD_2</td>\n",
       "      <td>HOUSEHOLD_2_516</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3049 rows × 1916 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_id      dept_id          item_id  2011-01-29  2011-01-30  \\\n",
       "0         FOODS      FOODS_1      FOODS_1_001           6           6   \n",
       "1         FOODS      FOODS_1      FOODS_1_002           4           5   \n",
       "2         FOODS      FOODS_1      FOODS_1_003          14           8   \n",
       "3         FOODS      FOODS_1      FOODS_1_004           0           0   \n",
       "4         FOODS      FOODS_1      FOODS_1_005          34          32   \n",
       "...         ...          ...              ...         ...         ...   \n",
       "3044  HOUSEHOLD  HOUSEHOLD_2  HOUSEHOLD_2_512           5           4   \n",
       "3045  HOUSEHOLD  HOUSEHOLD_2  HOUSEHOLD_2_513           0           0   \n",
       "3046  HOUSEHOLD  HOUSEHOLD_2  HOUSEHOLD_2_514           4           8   \n",
       "3047  HOUSEHOLD  HOUSEHOLD_2  HOUSEHOLD_2_515           0           0   \n",
       "3048  HOUSEHOLD  HOUSEHOLD_2  HOUSEHOLD_2_516           2           4   \n",
       "\n",
       "      2011-01-31  2011-02-01  2011-02-02  2011-02-03  2011-02-04  ...  \\\n",
       "0              4           6           7          18          10  ...   \n",
       "1              7           4           3           4           1  ...   \n",
       "2              3           6           3           8          13  ...   \n",
       "3              0           0           0           0           0  ...   \n",
       "4             13          20          10          21          18  ...   \n",
       "...          ...         ...         ...         ...         ...  ...   \n",
       "3044           1           3           2           4           2  ...   \n",
       "3045           0           0           0           0           0  ...   \n",
       "3046           2           1           1           2           3  ...   \n",
       "3047           0           0           0           0           0  ...   \n",
       "3048           0           3           1           2           2  ...   \n",
       "\n",
       "      2016-04-15  2016-04-16  2016-04-17  2016-04-18  2016-04-19  2016-04-20  \\\n",
       "0              4           4          30           7           5           3   \n",
       "1              5           9           4           1           3           5   \n",
       "2              7           3           5           6           3           4   \n",
       "3              0           0           0           0           0           0   \n",
       "4             16          14          14          18          18          27   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "3044           6           7           9          13          12           2   \n",
       "3045           9           3           3           3           2           4   \n",
       "3046           1           2           2           0           1           0   \n",
       "3047           1           3           2           0           0           1   \n",
       "3048           4           1           1           0           0           1   \n",
       "\n",
       "      2016-04-21  2016-04-22  2016-04-23  2016-04-24  \n",
       "0              6           2          16           6  \n",
       "1              5           3           3           1  \n",
       "2              4           3          11           5  \n",
       "3              0           0           0           0  \n",
       "4             12          15          38           9  \n",
       "...          ...         ...         ...         ...  \n",
       "3044           8           6          10           5  \n",
       "3045           6           7           4          11  \n",
       "3046           0           2           2           2  \n",
       "3047           1           1           5           1  \n",
       "3048           2           1           0           1  \n",
       "\n",
       "[3049 rows x 1916 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time related covariates: \n",
    "\n",
    "- Wehther it is weekend/not, we observed sales are high overweekend across three categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = [d_to_date[d] for d in sales_train_validation.columns[6:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statrting date is 2011-01-29\n",
      "The statrting date is 2016-04-24\n"
     ]
    }
   ],
   "source": [
    "print(f'The statrting date is {date[0]}')\n",
    "print(f'The statrting date is {date[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given data is weekday.\n"
     ]
    }
   ],
   "source": [
    "d = datetime.strptime(date[2], '%Y-%m-%d')\n",
    "if d.weekday() > 4:\n",
    "    print ('Given date is weekend.')\n",
    "else:\n",
    "    print ('Given data is weekday.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1913\n"
     ]
    }
   ],
   "source": [
    "weekend_binary = [1 if (datetime.strptime(d, '%Y-%m-%d')).weekday() > 4 else 0 for d in date]\n",
    "print(len(weekend_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1913, 30490)\n"
     ]
    }
   ],
   "source": [
    "stv = sales_train_validation[sales_train_validation.columns[6:]]\n",
    "stv = stv.T\n",
    "print(stv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1913"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stv.sum(axis=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1913, 30490, 1])\n"
     ]
    }
   ],
   "source": [
    "hie_index = torch.arange(stv.shape[1])\n",
    "\n",
    "hie_index_2d = hie_index.expand(stv.shape[0], stv.shape[1])\n",
    "\n",
    "hie_index_3d = hie_index_2d.reshape(\n",
    "    hie_index_2d.shape[0], hie_index_2d.shape[-1], 1\n",
    ")\n",
    "\n",
    "print(hie_index_3d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1913.0\n",
      "(1913, 30490, 1)\n",
      "torch.Size([1913, 30490, 1])\n"
     ]
    }
   ],
   "source": [
    "stv_proportions = np.divide(stv.values, stv.sum(axis=1).values.reshape(-1,1))\n",
    "print(stv_proportions.sum(axis=1).sum())\n",
    "\n",
    "stv_proportions_3d = stv_proportions.reshape(stv_proportions.shape[0], stv_proportions.shape[1], 1)\n",
    "print(stv_proportions_3d.shape)\n",
    "\n",
    "proportions_tensor = torch.tensor(stv_proportions_3d)\n",
    "print(proportions_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1913, 30490, 1])\n",
      "tensor([1])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weekend_binary_tensor = torch.tensor(weekend_binary).unsqueeze_(-1).unsqueeze_(-1)\n",
    "weekend_binary_tensor = weekend_binary_tensor.expand(stv_proportions.shape[0], stv_proportions.shape[1], weekend_binary_tensor.shape[-1])\n",
    "print(weekend_binary_tensor.shape)\n",
    "print(weekend_binary_tensor[1,0])\n",
    "print(weekend_binary_tensor[2,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1913, 30490, 1])\n"
     ]
    }
   ],
   "source": [
    "parent_sales_tensor = torch.tensor(parent_sales).unsqueeze_(-1).unsqueeze_(-1)\n",
    "parent_sales_tensor = parent_sales_tensor.expand(stv_proportions.shape[0], stv_proportions.shape[1], parent_sales_tensor.shape[-1])\n",
    "print(parent_sales_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1913, 30490, 4])\n"
     ]
    }
   ],
   "source": [
    "data_3d = torch.cat((proportions_tensor, parent_sales_tensor,weekend_binary_tensor, hie_index_3d), -1)\n",
    "data_3d = data_3d.double()\n",
    "print(data_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1858, 56, 30490, 4])\n",
      "torch.Size([56, 30490, 4])\n",
      "data correctly processed to generate time-bacted tensor\n",
      "torch.Size([1858, 28, 30490, 4])\n",
      "torch.Size([1858, 28, 30490, 1])\n",
      "Entering the training pipeline\n",
      "torch.Size([1858, 28, 30490, 4])\n",
      "torch.Size([1858, 28, 30490, 1])\n"
     ]
    }
   ],
   "source": [
    "# dimension about the dataset\n",
    "no_child = proportions_tensor.shape[1]\n",
    "History = 28\n",
    "Forward = 28\n",
    "\n",
    "number_observations = data_3d.shape[0] - (History + Forward) + 1\n",
    "\n",
    "data_3d_time_batched = torch.empty(\n",
    "    number_observations, History + Forward, data_3d.shape[1], data_3d.shape[2]\n",
    ")\n",
    "\n",
    "for i in range(number_observations):\n",
    "\n",
    "    data_3d_time_batched[i, :, :, :] = data_3d[i : i + History + Forward, :, :]\n",
    "\n",
    "print(data_3d_time_batched.shape)\n",
    "print(data_3d_time_batched[-1,:,:,:].shape)\n",
    "\n",
    "\n",
    "#if torch.equal(data_3d_time_batched[-1, -1, :, :].double(), data_3d[-1, :, :].double()):\n",
    "\n",
    "print(\"data correctly processed to generate time-bacted tensor\")\n",
    "\n",
    "input_tensor = torch.empty(\n",
    "    number_observations,\n",
    "    History,\n",
    "    data_3d_time_batched.shape[-2],\n",
    "    data_3d_time_batched.shape[-1],\n",
    ")\n",
    "\n",
    "## We first use the recursive predicitng mechanism in LSTM, in the future we release more blocks that adapt to teacher-forcing/mixed training\n",
    "target_tensor = torch.empty(\n",
    "    number_observations,\n",
    "    Forward,\n",
    "    data_3d_time_batched.shape[-2],\n",
    "    1\n",
    "    # data_3d_time_batched.shape[-1]\n",
    ")\n",
    "\n",
    "print(input_tensor.shape)\n",
    "print(target_tensor.shape)\n",
    "\n",
    "print(\"Entering the training pipeline\")\n",
    "\n",
    "for i in range(data_3d_time_batched.shape[0]):\n",
    "\n",
    "    input_tensor[i] = data_3d_time_batched[i, :History, :, :]\n",
    "    target_2d = data_3d_time_batched[i, History:, :, 0]\n",
    "    target_tensor[i] = target_2d.reshape(\n",
    "        target_2d.shape[0], target_2d.shape[1], 1\n",
    "    )\n",
    "\n",
    "    # print(input_tensor.shape)\n",
    "    # print(target_tensor.shape)\n",
    "\n",
    "print(input_tensor.shape)\n",
    "print(target_tensor.shape)\n",
    "    # print(target_tensor[-1,0,:,:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 12, got 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericliu/Desktop/ML-Side Project/TS Forecasting/Kaggle - M5 - Sales Forecating (accuracy)/notebook/feature_generation.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=38'>39</a>\u001b[0m l_r \u001b[39m=\u001b[39m \u001b[39m0.00079\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=40'>41</a>\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=41'>42</a>\u001b[0m p_model\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=42'>43</a>\u001b[0m     input_tensor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=43'>44</a>\u001b[0m     target_tensor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=44'>45</a>\u001b[0m     n_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=45'>46</a>\u001b[0m     target_len,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=46'>47</a>\u001b[0m     batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=47'>48</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49ml_r,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ericliu/Desktop/ML-Side%20Project/TS%20Forecasting/Kaggle%20-%20M5%20-%20Sales%20Forecating%20%28accuracy%29/notebook/feature_generation.ipynb#ch0000028?line=48'>49</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ML-Side Project/TS Forecasting/Kaggle - M5 - Sales Forecating (accuracy)/src/proption_model.py:425\u001b[0m, in \u001b[0;36mproportion_model.train\u001b[0;34m(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m cat((\u001b[39minput\u001b[39m, embedd_vector), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    419\u001b[0m \u001b[39m# print(f\"with ENCODED hierachy, input batch diemsion is {input.shape}\")\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[39m## ------------ lstm encoder ---input: L, C, input_dim ---------- ##\u001b[39;00m\n\u001b[1;32m    422\u001b[0m encoder_ouput, (\n\u001b[1;32m    423\u001b[0m     encoder_hn,\n\u001b[1;32m    424\u001b[0m     encoder_cn,\n\u001b[0;32m--> 425\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_lstm\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m, h0, c0)\n\u001b[1;32m    426\u001b[0m encoder_cache \u001b[39m=\u001b[39m (encoder_hn, encoder_cn)\n\u001b[1;32m    427\u001b[0m \u001b[39m# print(f\"The encoder final hidden state shape is {encoder_hn.shape}\")\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# print(f\"The encoder final cell state shape is {encoder_hn.shape}\")\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[39m## ------------- lstm ddecoder --------L, C, input_dim ----------- ##\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ML-Side Project/TS Forecasting/Kaggle - M5 - Sales Forecating (accuracy)/src/proption_model.py:119\u001b[0m, in \u001b[0;36mencoder_lstm.forward\u001b[0;34m(self, x, h0, c0)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h0, c0):\n\u001b[1;32m    110\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    : param x :                    input of shape (# in batch, seq_len, lstm_input_dim)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m    :                              element in the sequence\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     encoder_ouptut, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhn, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcn) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhn, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcn)\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m encoder_ouptut, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/rnn.py:759\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 759\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    760\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    762\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/rnn.py:684\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    680\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    681\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    682\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    683\u001b[0m                        ):\n\u001b[0;32m--> 684\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    685\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    686\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    687\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    688\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 12, got 11"
     ]
    }
   ],
   "source": [
    "###---------- dimension on the model hypter-parameters from the paper ------------ ######\n",
    "num_hts_embedd = no_child\n",
    "hts_embedd_dim = 8\n",
    "covariate_dim = 2 \n",
    "\n",
    "lstm_input_dim = 2 + covariate_dim + hts_embedd_dim\n",
    "lstm_hidden_dim = 48\n",
    "lstm_num_layer = 1\n",
    "lstm_output_dim = 64\n",
    "\n",
    "mha_embedd_dim = lstm_output_dim\n",
    "num_head = 4\n",
    "num_attention_layer = 1\n",
    "mha_output_dim = mha_embedd_dim\n",
    "residual_output_dim = mha_output_dim\n",
    "model_ouput_dim = 1\n",
    "\n",
    "# define the model object\n",
    "p_model = proportion_model(\n",
    "    num_hts_embedd,\n",
    "    hts_embedd_dim,  # ts embedding hyper pars\n",
    "    lstm_input_dim,\n",
    "    lstm_hidden_dim,\n",
    "    lstm_num_layer,\n",
    "    lstm_output_dim,  # lstm hyper pars\n",
    "    mha_embedd_dim,\n",
    "    num_head,\n",
    "    num_attention_layer,  # mha hyper pars\n",
    "    mha_output_dim,\n",
    "    residual_output_dim,  # skip connection hyper pars\n",
    "    model_ouput_dim,  # output later hyper pars\n",
    ")\n",
    "\n",
    "###---------- trainign parameters from the paper ------------ ######\n",
    "\n",
    "n_epochs = 50\n",
    "target_len = Forward\n",
    "batch_size = 4\n",
    "l_r = 0.00079\n",
    "\n",
    "# start training\n",
    "p_model.train(\n",
    "    input_tensor,\n",
    "    target_tensor,\n",
    "    n_epochs,\n",
    "    target_len,\n",
    "    batch_size,\n",
    "    learning_rate=l_r,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
